Pour k=1, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.09844523
Iteration 2, loss = 2.03452355
Iteration 3, loss = 1.94716207
Iteration 4, loss = 1.84688484
Iteration 5, loss = 1.71329243
Iteration 6, loss = 1.54409329
Iteration 7, loss = 1.42994818
Iteration 8, loss = 1.37987876
Iteration 9, loss = 1.32950624
Iteration 10, loss = 1.27914582
Iteration 11, loss = 1.23917402
Iteration 12, loss = 1.20391983
Iteration 13, loss = 1.17323994
Iteration 14, loss = 1.14647837
Iteration 15, loss = 1.12302718
Iteration 16, loss = 1.10260551
Iteration 17, loss = 1.08373055
Iteration 18, loss = 1.06627835
Iteration 19, loss = 1.04999081
Iteration 20, loss = 1.03474616
Iteration 21, loss = 1.02041980
Iteration 22, loss = 1.00663720
Iteration 23, loss = 0.99377785
Iteration 24, loss = 0.98155278
Iteration 25, loss = 0.96990464
Iteration 26, loss = 0.95882406
Iteration 27, loss = 0.94825711
Iteration 28, loss = 0.93825225
Iteration 29, loss = 0.92848803
Iteration 30, loss = 0.91926251
Iteration 31, loss = 0.91060922
Iteration 32, loss = 0.90210036
Iteration 33, loss = 0.89460959
Iteration 34, loss = 0.88617225
Iteration 35, loss = 0.87864849
Iteration 36, loss = 0.87146914
Iteration 37, loss = 0.86463752
Iteration 38, loss = 0.85814893
Iteration 39, loss = 0.85173007
Iteration 40, loss = 0.84555188
Iteration 41, loss = 0.83962661
Iteration 42, loss = 0.83389525
Iteration 43, loss = 0.82837481
Iteration 44, loss = 0.82303667
Iteration 45, loss = 0.81785392
Iteration 46, loss = 0.81298248
Iteration 47, loss = 0.80805002
Iteration 48, loss = 0.80332631
Iteration 49, loss = 0.79876207
Iteration 50, loss = 0.79442078
Iteration 51, loss = 0.79006407
Iteration 52, loss = 0.78591497
Iteration 53, loss = 0.78198762
Iteration 54, loss = 0.77798382
Iteration 55, loss = 0.77421265
Iteration 56, loss = 0.77050593
Iteration 57, loss = 0.76698272
Iteration 58, loss = 0.76353315
Iteration 59, loss = 0.76038065
Iteration 60, loss = 0.75675833
Iteration 61, loss = 0.75362231
Iteration 62, loss = 0.75081955
Iteration 63, loss = 0.74742572
Iteration 64, loss = 0.74465918
Iteration 65, loss = 0.74191031
Iteration 66, loss = 0.73886161
Iteration 67, loss = 0.73650522
Iteration 68, loss = 0.73352196
Iteration 69, loss = 0.73100372
Iteration 70, loss = 0.72850906
Iteration 71, loss = 0.72625149
Iteration 72, loss = 0.72365886
Iteration 73, loss = 0.72157869
Iteration 74, loss = 0.71905229
Iteration 75, loss = 0.71714737
Iteration 76, loss = 0.71467493
Iteration 77, loss = 0.71262022
Iteration 78, loss = 0.71069463
Iteration 79, loss = 0.70869731
Iteration 80, loss = 0.70672474
Iteration 81, loss = 0.70484587
Iteration 82, loss = 0.70295077
Iteration 83, loss = 0.70117364
Iteration 84, loss = 0.69931861
Iteration 85, loss = 0.69768132
Iteration 86, loss = 0.69589361
Iteration 87, loss = 0.69431587
Iteration 88, loss = 0.69260169
Iteration 89, loss = 0.69109136
Iteration 90, loss = 0.68946181
Iteration 91, loss = 0.68799769
Iteration 92, loss = 0.68645625
Iteration 93, loss = 0.68503388
Iteration 94, loss = 0.68356362
Iteration 95, loss = 0.68220034
Iteration 96, loss = 0.68080292
Iteration 97, loss = 0.67945688
Iteration 98, loss = 0.67815593
Iteration 99, loss = 0.67683245
Iteration 100, loss = 0.67559466
Iteration 101, loss = 0.67431887
Iteration 102, loss = 0.67313973
Iteration 103, loss = 0.67189095
Iteration 104, loss = 0.67078006
Iteration 105, loss = 0.66955566
Iteration 106, loss = 0.66850212
Iteration 107, loss = 0.66730547
Iteration 108, loss = 0.66615931
Iteration 109, loss = 0.66524741
Iteration 110, loss = 0.66409930
Iteration 111, loss = 0.66314109
Iteration 112, loss = 0.66204506
Iteration 113, loss = 0.66114120
Iteration 114, loss = 0.66005281
Iteration 115, loss = 0.65920476
Iteration 116, loss = 0.65814659
Iteration 117, loss = 0.65719325
Iteration 118, loss = 0.65642543
Iteration 119, loss = 0.65544396
Iteration 120, loss = 0.65460558
Iteration 121, loss = 0.65369769
Iteration 122, loss = 0.65285074
Iteration 123, loss = 0.65201386
Iteration 124, loss = 0.65115064
Iteration 125, loss = 0.65038011
Iteration 126, loss = 0.64951533
Iteration 127, loss = 0.64879180
Iteration 128, loss = 0.64793122
Iteration 129, loss = 0.64725232
Iteration 130, loss = 0.64641041
Iteration 131, loss = 0.64565262
Iteration 132, loss = 0.64502967
Iteration 133, loss = 0.64424463
Iteration 134, loss = 0.64356710
Iteration 135, loss = 0.64283853
Iteration 136, loss = 0.64215705
Iteration 137, loss = 0.64147778
Iteration 138, loss = 0.64078732
Iteration 139, loss = 0.64015286
Iteration 140, loss = 0.63945938
Iteration 141, loss = 0.63886324
Iteration 142, loss = 0.63817763
Iteration 143, loss = 0.63760489
Iteration 144, loss = 0.63693390
Iteration 145, loss = 0.63630879
Iteration 146, loss = 0.63571469
Iteration 147, loss = 0.63512159
Iteration 148, loss = 0.63454206
Iteration 149, loss = 0.63396251
Iteration 150, loss = 0.63345760
Iteration 151, loss = 0.63285739
Iteration 152, loss = 0.63233469
Iteration 153, loss = 0.63175376
Iteration 154, loss = 0.63125107
Iteration 155, loss = 0.63068221
Iteration 156, loss = 0.63019036
Iteration 157, loss = 0.62963997
Iteration 158, loss = 0.62915573
Iteration 159, loss = 0.62862217
Iteration 160, loss = 0.62814822
Iteration 161, loss = 0.62763205
Iteration 162, loss = 0.62716364
Iteration 163, loss = 0.62666522
Iteration 164, loss = 0.62620345
Iteration 165, loss = 0.62572153
Iteration 166, loss = 0.62526716
Iteration 167, loss = 0.62480226
Iteration 168, loss = 0.62435154
Iteration 169, loss = 0.62390433
Iteration 170, loss = 0.62345783
Iteration 171, loss = 0.62302508
Iteration 172, loss = 0.62258590
Iteration 173, loss = 0.62216861
Iteration 174, loss = 0.62173247
Iteration 175, loss = 0.62133084
Iteration 176, loss = 0.62090219
Iteration 177, loss = 0.62048745
Iteration 178, loss = 0.62007829
Iteration 179, loss = 0.61970205
Iteration 180, loss = 0.61930430
Iteration 181, loss = 0.61890531
Iteration 182, loss = 0.61853244
Iteration 183, loss = 0.61813209
Iteration 184, loss = 0.61777497
Iteration 185, loss = 0.61737806
Iteration 186, loss = 0.61700649
Iteration 187, loss = 0.61665331
Iteration 188, loss = 0.61629828
Iteration 189, loss = 0.61592119
Iteration 190, loss = 0.61558842
Iteration 191, loss = 0.61521485
Iteration 192, loss = 0.61487061
Iteration 193, loss = 0.61452977
Iteration 194, loss = 0.61420500
Iteration 195, loss = 0.61384669
Iteration 196, loss = 0.61352690
Iteration 197, loss = 0.61319707
Iteration 198, loss = 0.61285767
Iteration 199, loss = 0.61253598
Iteration 200, loss = 0.61221706
Iteration 201, loss = 0.61191413
Iteration 202, loss = 0.61158406
Iteration 203, loss = 0.61127558
Iteration 204, loss = 0.61097775
Iteration 205, loss = 0.61065959
Iteration 206, loss = 0.61035968
Iteration 207, loss = 0.61005865
Iteration 208, loss = 0.60976148
Iteration 209, loss = 0.60947002
Iteration 210, loss = 0.60917638
Iteration 211, loss = 0.60888991
Iteration 212, loss = 0.60861491
Iteration 213, loss = 0.60832212
Iteration 214, loss = 0.60804029
Iteration 215, loss = 0.60776335
Iteration 216, loss = 0.60748909
Iteration 217, loss = 0.60721461
Iteration 218, loss = 0.60694412
Iteration 219, loss = 0.60667677
Iteration 220, loss = 0.60641260
Iteration 221, loss = 0.60614822
Iteration 222, loss = 0.60588724
Iteration 223, loss = 0.60562851
Iteration 224, loss = 0.60537214
Iteration 225, loss = 0.60511803
Iteration 226, loss = 0.60486625
Iteration 227, loss = 0.60461650
Iteration 228, loss = 0.60436884
Iteration 229, loss = 0.60412320
Iteration 230, loss = 0.60387962
Iteration 231, loss = 0.60363811
Iteration 232, loss = 0.60339853
Iteration 233, loss = 0.60316079
Iteration 234, loss = 0.60292500
Iteration 235, loss = 0.60269105
Iteration 236, loss = 0.60245911
Iteration 237, loss = 0.60222887
Iteration 238, loss = 0.60200043
Iteration 239, loss = 0.60177380
Iteration 240, loss = 0.60154887
Iteration 241, loss = 0.60132577
Iteration 242, loss = 0.60110430
Iteration 243, loss = 0.60088446
Iteration 244, loss = 0.60066629
Iteration 245, loss = 0.60044974
Iteration 246, loss = 0.60023477
Iteration 247, loss = 0.60002148
Iteration 248, loss = 0.59980962
Iteration 249, loss = 0.59959929
Iteration 250, loss = 0.59939048
Iteration 251, loss = 0.59918313
Iteration 252, loss = 0.59897727
Iteration 253, loss = 0.59877290
Iteration 254, loss = 0.59856984
Iteration 255, loss = 0.59836819
Iteration 256, loss = 0.59816792
Iteration 257, loss = 0.59796898
Iteration 258, loss = 0.59777135
Iteration 259, loss = 0.59757517
Iteration 260, loss = 0.59738011
Iteration 261, loss = 0.59718634
Iteration 262, loss = 0.59699385
Iteration 263, loss = 0.59680252
Iteration 264, loss = 0.59661244
Iteration 265, loss = 0.59642353
Iteration 266, loss = 0.59623581
Iteration 267, loss = 0.59604920
Iteration 268, loss = 0.59586370
Iteration 269, loss = 0.59567928
Iteration 270, loss = 0.59549597
Iteration 271, loss = 0.59531369
Iteration 272, loss = 0.59513249
Iteration 273, loss = 0.59495233
Iteration 274, loss = 0.59477312
Iteration 275, loss = 0.59459491
Iteration 276, loss = 0.59441736
Iteration 277, loss = 0.59424038
Iteration 278, loss = 0.59406440
Iteration 279, loss = 0.59388938
Iteration 280, loss = 0.59371521
Iteration 281, loss = 0.59354190
Iteration 282, loss = 0.59336949
Iteration 283, loss = 0.59319788
Iteration 284, loss = 0.59302706
Iteration 285, loss = 0.59285705
Iteration 286, loss = 0.59268776
Iteration 287, loss = 0.59251925
Iteration 288, loss = 0.59235148
Iteration 289, loss = 0.59218442
Iteration 290, loss = 0.59201806
Iteration 291, loss = 0.59185243
Iteration 292, loss = 0.59168740
Iteration 293, loss = 0.59152301
Iteration 294, loss = 0.59135923
Iteration 295, loss = 0.59119606
Iteration 296, loss = 0.59103346
Iteration 297, loss = 0.59087144
Iteration 298, loss = 0.59070999
Iteration 299, loss = 0.59054906
Iteration 300, loss = 0.59038862
Iteration 301, loss = 0.59022867
Iteration 302, loss = 0.59006921
Iteration 303, loss = 0.58991019
Iteration 304, loss = 0.58975163
Iteration 305, loss = 0.58959346
Iteration 306, loss = 0.58943579
Iteration 307, loss = 0.58927840
Iteration 308, loss = 0.58912144
Iteration 309, loss = 0.58896479
Iteration 310, loss = 0.58880850
Iteration 311, loss = 0.58865251
Iteration 312, loss = 0.58849684
Iteration 313, loss = 0.58834144
Iteration 314, loss = 0.58818637
Iteration 315, loss = 0.58803148
Iteration 316, loss = 0.58787684
Iteration 317, loss = 0.58772242
Iteration 318, loss = 0.58756819
Iteration 319, loss = 0.58741416
Iteration 320, loss = 0.58726028
Iteration 321, loss = 0.58710666
Iteration 322, loss = 0.58695318
Iteration 323, loss = 0.58679979
Iteration 324, loss = 0.58664664
Iteration 325, loss = 0.58649334
Iteration 326, loss = 0.58634023
Iteration 327, loss = 0.58618723
Iteration 328, loss = 0.58603411
Iteration 329, loss = 0.58588117
Iteration 330, loss = 0.58572819
Iteration 331, loss = 0.58557515
Iteration 332, loss = 0.58542213
Iteration 333, loss = 0.58526918
Iteration 334, loss = 0.58511609
Iteration 335, loss = 0.58496287
Iteration 336, loss = 0.58480974
Iteration 337, loss = 0.58465641
Iteration 338, loss = 0.58450293
Iteration 339, loss = 0.58434953
Iteration 340, loss = 0.58419581
Iteration 341, loss = 0.58404195
Iteration 342, loss = 0.58388803
Iteration 343, loss = 0.58373395
Iteration 344, loss = 0.58357961
Iteration 345, loss = 0.58342511
Iteration 346, loss = 0.58327053
Iteration 347, loss = 0.58311559
Iteration 348, loss = 0.58296039
Iteration 349, loss = 0.58280506
Iteration 350, loss = 0.58264941
Iteration 351, loss = 0.58249346
Iteration 352, loss = 0.58233736
Iteration 353, loss = 0.58218090
Iteration 354, loss = 0.58202410
Iteration 355, loss = 0.58186718
Iteration 356, loss = 0.58170973
Iteration 357, loss = 0.58155195
Iteration 358, loss = 0.58139406
Iteration 359, loss = 0.58123559
Iteration 360, loss = 0.58107685
Iteration 361, loss = 0.58091782
Iteration 362, loss = 0.58075827
Iteration 363, loss = 0.58059841
Iteration 364, loss = 0.58043824
Iteration 365, loss = 0.58027753
Iteration 366, loss = 0.58011649
Iteration 367, loss = 0.57995509
Iteration 368, loss = 0.57979311
Iteration 369, loss = 0.57963085
Iteration 370, loss = 0.57946810
Iteration 371, loss = 0.57930485
Iteration 372, loss = 0.57914119
Iteration 373, loss = 0.57897716
Iteration 374, loss = 0.57881257
Iteration 375, loss = 0.57864749
Iteration 376, loss = 0.57848199
Iteration 377, loss = 0.57831597
Iteration 378, loss = 0.57814949
Iteration 379, loss = 0.57798253
Iteration 380, loss = 0.57781509
Iteration 381, loss = 0.57764718
Iteration 382, loss = 0.57747866
Iteration 383, loss = 0.57730966
Iteration 384, loss = 0.57714023
Iteration 385, loss = 0.57697023
Iteration 386, loss = 0.57679964
Iteration 387, loss = 0.57662873
Iteration 388, loss = 0.57645708
Iteration 389, loss = 0.57628493
Iteration 390, loss = 0.57611242
Iteration 391, loss = 0.57593918
Iteration 392, loss = 0.57576554
Iteration 393, loss = 0.57559133
Iteration 394, loss = 0.57541653
Iteration 395, loss = 0.57524134
Iteration 396, loss = 0.57506544
Iteration 397, loss = 0.57488903
Iteration 398, loss = 0.57471216
Iteration 399, loss = 0.57453468
Iteration 400, loss = 0.57435664
Iteration 401, loss = 0.57417811
Iteration 402, loss = 0.57399903
Iteration 403, loss = 0.57381946
Iteration 404, loss = 0.57363929
Iteration 405, loss = 0.57345857
Iteration 406, loss = 0.57327741
Iteration 407, loss = 0.57309553
Iteration 408, loss = 0.57291325
Iteration 409, loss = 0.57273036
Iteration 410, loss = 0.57254701
Iteration 411, loss = 0.57236314
Iteration 412, loss = 0.57217865
Iteration 413, loss = 0.57200115
Iteration 414, loss = 0.57631963
Iteration 415, loss = 0.57191376
Iteration 416, loss = 0.57170585
Iteration 417, loss = 0.57136813
Iteration 418, loss = 0.57116327
Iteration 419, loss = 0.57097194
Iteration 420, loss = 0.57078229
Iteration 421, loss = 0.57059134
Iteration 422, loss = 0.57040044
Iteration 423, loss = 0.57020965
Iteration 424, loss = 0.57001865
Iteration 425, loss = 0.56982761
Iteration 426, loss = 0.56963609
Iteration 427, loss = 0.56944411
Iteration 428, loss = 0.56925173
Iteration 429, loss = 0.56905892
Iteration 430, loss = 0.56886571
Iteration 431, loss = 0.56867208
Iteration 432, loss = 0.56847805
Iteration 433, loss = 0.56828364
Iteration 434, loss = 0.56808886
Iteration 435, loss = 0.56789364
Iteration 436, loss = 0.56769803
Iteration 437, loss = 0.56750745
Iteration 438, loss = 0.56622664
Iteration 439, loss = 0.56597430
Iteration 440, loss = 0.56719167
Iteration 441, loss = 0.57686871
Iteration 442, loss = 0.56463451
Iteration 443, loss = 0.56539945
Iteration 444, loss = 0.56702519
Iteration 445, loss = 0.56534014
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.454545454545
Mean squared error = 2.36363636364
Mean absolute error = 0.909090909091
Median absolute error = 0.0
k_error=2.36363636364
____________________Fin K=1________________________

Pour k=2, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.12089259
Iteration 2, loss = 2.06761856
Iteration 3, loss = 1.99488348
Iteration 4, loss = 1.91734966
Iteration 5, loss = 1.83160609
Iteration 6, loss = 1.72731946
Iteration 7, loss = 1.60016356
Iteration 8, loss = 1.52161482
Iteration 9, loss = 1.47980059
Iteration 10, loss = 1.43171466
Iteration 11, loss = 1.38418567
Iteration 12, loss = 1.34412164
Iteration 13, loss = 1.30721976
Iteration 14, loss = 1.27491237
Iteration 15, loss = 1.24632372
Iteration 16, loss = 1.22119063
Iteration 17, loss = 1.19855238
Iteration 18, loss = 1.17786335
Iteration 19, loss = 1.15866138
Iteration 20, loss = 1.14071892
Iteration 21, loss = 1.12389896
Iteration 22, loss = 1.10805724
Iteration 23, loss = 1.09313047
Iteration 24, loss = 1.07914074
Iteration 25, loss = 1.06598526
Iteration 26, loss = 1.05349634
Iteration 27, loss = 1.04155309
Iteration 28, loss = 1.03011428
Iteration 29, loss = 1.01917732
Iteration 30, loss = 1.00888481
Iteration 31, loss = 0.99936372
Iteration 32, loss = 0.98939020
Iteration 33, loss = 0.98043431
Iteration 34, loss = 0.97128112
Iteration 35, loss = 0.96297601
Iteration 36, loss = 0.95453161
Iteration 37, loss = 0.94704480
Iteration 38, loss = 0.93975207
Iteration 39, loss = 0.93323360
Iteration 40, loss = 0.92474240
Iteration 41, loss = 0.91940175
Iteration 42, loss = 0.91095113
Iteration 43, loss = 0.90472992
Iteration 44, loss = 0.89784108
Iteration 45, loss = 0.89151568
Iteration 46, loss = 0.88557372
Iteration 47, loss = 0.88033587
Iteration 48, loss = 0.87448713
Iteration 49, loss = 0.87158561
Iteration 50, loss = 0.86410261
Iteration 51, loss = 0.85858489
Iteration 52, loss = 0.85362525
Iteration 53, loss = 0.84898180
Iteration 54, loss = 0.84505991
Iteration 55, loss = 0.83989721
Iteration 56, loss = 0.83533049
Iteration 57, loss = 0.83117189
Iteration 58, loss = 0.82695121
Iteration 59, loss = 0.82295761
Iteration 60, loss = 0.81910327
Iteration 61, loss = 0.81542593
Iteration 62, loss = 0.81231241
Iteration 63, loss = 0.81204883
Iteration 64, loss = 0.80580325
Iteration 65, loss = 0.80176279
Iteration 66, loss = 0.79836952
Iteration 67, loss = 0.79528402
Iteration 68, loss = 0.79214482
Iteration 69, loss = 0.78938874
Iteration 70, loss = 0.78850521
Iteration 71, loss = 0.78399482
Iteration 72, loss = 0.78083586
Iteration 73, loss = 0.77814700
Iteration 74, loss = 0.77564623
Iteration 75, loss = 0.77324220
Iteration 76, loss = 0.77136458
Iteration 77, loss = 0.76850801
Iteration 78, loss = 0.76603487
Iteration 79, loss = 0.76394120
Iteration 80, loss = 0.76165522
Iteration 81, loss = 0.75951151
Iteration 82, loss = 0.75743799
Iteration 83, loss = 0.75547634
Iteration 84, loss = 0.75359922
Iteration 85, loss = 0.75160084
Iteration 86, loss = 0.74976398
Iteration 87, loss = 0.74803860
Iteration 88, loss = 0.74711113
Iteration 89, loss = 0.74736215
Iteration 90, loss = 0.74334426
Iteration 91, loss = 0.74133057
Iteration 92, loss = 0.73984634
Iteration 93, loss = 0.73901257
Iteration 94, loss = 0.73657556
Iteration 95, loss = 0.73511095
Iteration 96, loss = 0.73361235
Iteration 97, loss = 0.73219836
Iteration 98, loss = 0.73083448
Iteration 99, loss = 0.73129452
Iteration 100, loss = 0.72824917
Iteration 101, loss = 0.72673657
Iteration 102, loss = 0.72534350
Iteration 103, loss = 0.72437538
Iteration 104, loss = 0.72402925
Iteration 105, loss = 0.72169663
Iteration 106, loss = 0.72041857
Iteration 107, loss = 0.71936003
Iteration 108, loss = 0.71814245
Iteration 109, loss = 0.71709506
Iteration 110, loss = 0.71614142
Iteration 111, loss = 0.71486026
Iteration 112, loss = 0.71372266
Iteration 113, loss = 0.71264808
Iteration 114, loss = 0.71160797
Iteration 115, loss = 0.71058717
Iteration 116, loss = 0.70958724
Iteration 117, loss = 0.70877960
Iteration 118, loss = 0.70779157
Iteration 119, loss = 0.70684558
Iteration 120, loss = 0.70601948
Iteration 121, loss = 0.70733085
Iteration 122, loss = 0.70443193
Iteration 123, loss = 0.70345337
Iteration 124, loss = 0.70246036
Iteration 125, loss = 0.70153010
Iteration 126, loss = 0.70068299
Iteration 127, loss = 0.69992897
Iteration 128, loss = 0.69909538
Iteration 129, loss = 0.69833291
Iteration 130, loss = 0.69757003
Iteration 131, loss = 0.69676343
Iteration 132, loss = 0.69597671
Iteration 133, loss = 0.69520472
Iteration 134, loss = 0.69446276
Iteration 135, loss = 0.69373294
Iteration 136, loss = 0.69301711
Iteration 137, loss = 0.69233624
Iteration 138, loss = 0.69190372
Iteration 139, loss = 0.69144764
Iteration 140, loss = 0.69032009
Iteration 141, loss = 0.68965206
Iteration 142, loss = 0.68900829
Iteration 143, loss = 0.68853265
Iteration 144, loss = 0.68880942
Iteration 145, loss = 0.68733525
Iteration 146, loss = 0.68667460
Iteration 147, loss = 0.68599127
Iteration 148, loss = 0.68535557
Iteration 149, loss = 0.68480945
Iteration 150, loss = 0.68419377
Iteration 151, loss = 0.68395496
Iteration 152, loss = 0.68306341
Iteration 153, loss = 0.68250121
Iteration 154, loss = 0.68194615
Iteration 155, loss = 0.68140333
Iteration 156, loss = 0.68092194
Iteration 157, loss = 0.68035443
Iteration 158, loss = 0.67982208
Iteration 159, loss = 0.67930433
Iteration 160, loss = 0.67879625
Iteration 161, loss = 0.67829587
Iteration 162, loss = 0.67787313
Iteration 163, loss = 0.67732665
Iteration 164, loss = 0.67683484
Iteration 165, loss = 0.67638252
Iteration 166, loss = 0.67594767
Iteration 167, loss = 0.67543861
Iteration 168, loss = 0.67497426
Iteration 169, loss = 0.67452131
Iteration 170, loss = 0.67407513
Iteration 171, loss = 0.67365687
Iteration 172, loss = 0.67327859
Iteration 173, loss = 0.67351328
Iteration 174, loss = 0.67252773
Iteration 175, loss = 0.67207391
Iteration 176, loss = 0.67168197
Iteration 177, loss = 0.67130629
Iteration 178, loss = 0.67078681
Iteration 179, loss = 0.67036365
Iteration 180, loss = 0.66994905
Iteration 181, loss = 0.66956089
Iteration 182, loss = 0.66920170
Iteration 183, loss = 0.66879409
Iteration 184, loss = 0.66841244
Iteration 185, loss = 0.66803800
Iteration 186, loss = 0.66766836
Iteration 187, loss = 0.66730584
Iteration 188, loss = 0.66694290
Iteration 189, loss = 0.66659033
Iteration 190, loss = 0.66625800
Iteration 191, loss = 0.66588555
Iteration 192, loss = 0.66553971
Iteration 193, loss = 0.66519938
Iteration 194, loss = 0.66486137
Iteration 195, loss = 0.66452587
Iteration 196, loss = 0.66419503
Iteration 197, loss = 0.66386799
Iteration 198, loss = 0.66362923
Iteration 199, loss = 0.66341219
Iteration 200, loss = 0.66292644
Iteration 201, loss = 0.66275067
Iteration 202, loss = 0.66283374
Iteration 203, loss = 0.66214953
Iteration 204, loss = 0.66180531
Iteration 205, loss = 0.66147819
Iteration 206, loss = 0.66114701
Iteration 207, loss = 0.66082757
Iteration 208, loss = 0.66052265
Iteration 209, loss = 0.66023082
Iteration 210, loss = 0.65994261
Iteration 211, loss = 0.65965792
Iteration 212, loss = 0.65937701
Iteration 213, loss = 0.65909870
Iteration 214, loss = 0.65882350
Iteration 215, loss = 0.65855093
Iteration 216, loss = 0.65828105
Iteration 217, loss = 0.65801375
Iteration 218, loss = 0.65774901
Iteration 219, loss = 0.65748680
Iteration 220, loss = 0.65722707
Iteration 221, loss = 0.65696976
Iteration 222, loss = 0.65671952
Iteration 223, loss = 0.65660761
Iteration 224, loss = 0.65623404
Iteration 225, loss = 0.65598715
Iteration 226, loss = 0.65572870
Iteration 227, loss = 0.65548292
Iteration 228, loss = 0.65524080
Iteration 229, loss = 0.65500073
Iteration 230, loss = 0.65476292
Iteration 231, loss = 0.65452731
Iteration 232, loss = 0.65429383
Iteration 233, loss = 0.65406240
Iteration 234, loss = 0.65383300
Iteration 235, loss = 0.65362340
Iteration 236, loss = 0.65381755
Iteration 237, loss = 0.65329016
Iteration 238, loss = 0.65303354
Iteration 239, loss = 0.65279824
Iteration 240, loss = 0.65255721
Iteration 241, loss = 0.65232183
Iteration 242, loss = 0.65209831
Iteration 243, loss = 0.65188220
Iteration 244, loss = 0.65166918
Iteration 245, loss = 0.65145894
Iteration 246, loss = 0.65126862
Iteration 247, loss = 0.65116419
Iteration 248, loss = 0.65086328
Iteration 249, loss = 0.65065478
Iteration 250, loss = 0.65045303
Iteration 251, loss = 0.65025245
Iteration 252, loss = 0.65005445
Iteration 253, loss = 0.64985820
Iteration 254, loss = 0.64966388
Iteration 255, loss = 0.64947142
Iteration 256, loss = 0.64928050
Iteration 257, loss = 0.64909109
Iteration 258, loss = 0.64890324
Iteration 259, loss = 0.64871709
Iteration 260, loss = 0.64853236
Iteration 261, loss = 0.64834905
Iteration 262, loss = 0.64816714
Iteration 263, loss = 0.64799340
Iteration 264, loss = 0.64820436
Iteration 265, loss = 0.64776622
Iteration 266, loss = 0.64754573
Iteration 267, loss = 0.64735758
Iteration 268, loss = 0.64716378
Iteration 269, loss = 0.64697680
Iteration 270, loss = 0.64679553
Iteration 271, loss = 0.64663118
Iteration 272, loss = 0.64656312
Iteration 273, loss = 0.64629652
Iteration 274, loss = 0.64612594
Iteration 275, loss = 0.64595832
Iteration 276, loss = 0.64579196
Iteration 277, loss = 0.64562778
Iteration 278, loss = 0.64546503
Iteration 279, loss = 0.64530353
Iteration 280, loss = 0.64514277
Iteration 281, loss = 0.64497692
Iteration 282, loss = 0.64481486
Iteration 283, loss = 0.64465406
Iteration 284, loss = 0.64449470
Iteration 285, loss = 0.64433652
Iteration 286, loss = 0.64421046
Iteration 287, loss = 0.64442782
Iteration 288, loss = 0.64400223
Iteration 289, loss = 0.64381192
Iteration 290, loss = 0.64365687
Iteration 291, loss = 0.64350407
Iteration 292, loss = 0.64344347
Iteration 293, loss = 0.64321337
Iteration 294, loss = 0.64305488
Iteration 295, loss = 0.64290060
Iteration 296, loss = 0.64273937
Iteration 297, loss = 0.64258330
Iteration 298, loss = 0.64243087
Iteration 299, loss = 0.64228160
Iteration 300, loss = 0.64213440
Iteration 301, loss = 0.64198865
Iteration 302, loss = 0.64184396
Iteration 303, loss = 0.64170007
Iteration 304, loss = 0.64155684
Iteration 305, loss = 0.64141422
Iteration 306, loss = 0.64127215
Iteration 307, loss = 0.64113638
Iteration 308, loss = 0.64111992
Iteration 309, loss = 0.64086489
Iteration 310, loss = 0.64075602
Iteration 311, loss = 0.64106642
Iteration 312, loss = 0.64062567
Iteration 313, loss = 0.64044233
Iteration 314, loss = 0.64028111
Iteration 315, loss = 0.64011618
Iteration 316, loss = 0.63996009
Iteration 317, loss = 0.63981229
Iteration 318, loss = 0.63966978
Iteration 319, loss = 0.63953089
Iteration 320, loss = 0.63939411
Iteration 321, loss = 0.63925846
Iteration 322, loss = 0.63912671
Iteration 323, loss = 0.63912743
Iteration 324, loss = 0.63887019
Iteration 325, loss = 0.63873929
Iteration 326, loss = 0.63860355
Iteration 327, loss = 0.63846991
Iteration 328, loss = 0.63833675
Iteration 329, loss = 0.63820374
Iteration 330, loss = 0.63807092
Iteration 331, loss = 0.63793826
Iteration 332, loss = 0.63780575
Iteration 333, loss = 0.63767337
Iteration 334, loss = 0.63754112
Iteration 335, loss = 0.63740898
Iteration 336, loss = 0.63727692
Iteration 337, loss = 0.63714493
Iteration 338, loss = 0.63703535
Iteration 339, loss = 0.63767988
Iteration 340, loss = 0.63697414
Iteration 341, loss = 0.63681063
Iteration 342, loss = 0.63670581
Iteration 343, loss = 0.63646488
Iteration 344, loss = 0.63631525
Iteration 345, loss = 0.63616998
Iteration 346, loss = 0.63603203
Iteration 347, loss = 0.63589746
Iteration 348, loss = 0.63576451
Iteration 349, loss = 0.63563229
Iteration 350, loss = 0.63550030
Iteration 351, loss = 0.63536832
Iteration 352, loss = 0.63523626
Iteration 353, loss = 0.63510408
Iteration 354, loss = 0.63497180
Iteration 355, loss = 0.63483831
Iteration 356, loss = 0.63478405
Iteration 357, loss = 0.63611676
Iteration 358, loss = 0.63456339
Iteration 359, loss = 0.63440663
Iteration 360, loss = 0.63417075
Iteration 361, loss = 0.63392578
Iteration 362, loss = 0.63372696
Iteration 363, loss = 0.63354491
Iteration 364, loss = 0.63336964
Iteration 365, loss = 0.63319518
Iteration 366, loss = 0.63301938
Iteration 367, loss = 0.63284178
Iteration 368, loss = 0.63266240
Iteration 369, loss = 0.63248126
Iteration 370, loss = 0.63229831
Iteration 371, loss = 0.63211348
Iteration 372, loss = 0.63192669
Iteration 373, loss = 0.63173790
Iteration 374, loss = 0.63154707
Iteration 375, loss = 0.63135415
Iteration 376, loss = 0.63115920
Iteration 377, loss = 0.63096216
Iteration 378, loss = 0.63076406
Iteration 379, loss = 0.63056870
Iteration 380, loss = 0.63062283
Iteration 381, loss = 0.63066415
Iteration 382, loss = 0.63274113
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.2
Mean squared error = 1.0
Mean absolute error = 0.4
Median absolute error = 0.0
k_error=3.36363636364
____________________Fin K=2________________________

Pour k=3, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.11655710
Iteration 2, loss = 2.06346306
Iteration 3, loss = 1.99083686
Iteration 4, loss = 1.91223856
Iteration 5, loss = 1.82365513
Iteration 6, loss = 1.71241879
Iteration 7, loss = 1.58713610
Iteration 8, loss = 1.50898241
Iteration 9, loss = 1.46296263
Iteration 10, loss = 1.41221409
Iteration 11, loss = 1.36737293
Iteration 12, loss = 1.32891392
Iteration 13, loss = 1.29552807
Iteration 14, loss = 1.26646178
Iteration 15, loss = 1.24042477
Iteration 16, loss = 1.21788572
Iteration 17, loss = 1.19761119
Iteration 18, loss = 1.17871992
Iteration 19, loss = 1.16121461
Iteration 20, loss = 1.14473292
Iteration 21, loss = 1.12917028
Iteration 22, loss = 1.11452478
Iteration 23, loss = 1.10079215
Iteration 24, loss = 1.08787636
Iteration 25, loss = 1.07535974
Iteration 26, loss = 1.06353953
Iteration 27, loss = 1.05237357
Iteration 28, loss = 1.04178339
Iteration 29, loss = 1.03167902
Iteration 30, loss = 1.02194801
Iteration 31, loss = 1.01285526
Iteration 32, loss = 1.00445362
Iteration 33, loss = 0.99607694
Iteration 34, loss = 0.98752839
Iteration 35, loss = 0.97967152
Iteration 36, loss = 0.97205354
Iteration 37, loss = 0.96471485
Iteration 38, loss = 0.95746715
Iteration 39, loss = 0.95072798
Iteration 40, loss = 0.94376860
Iteration 41, loss = 0.93725840
Iteration 42, loss = 0.93090823
Iteration 43, loss = 0.92478662
Iteration 44, loss = 0.91885013
Iteration 45, loss = 0.91336196
Iteration 46, loss = 0.90744614
Iteration 47, loss = 0.90193507
Iteration 48, loss = 0.89659105
Iteration 49, loss = 0.89148340
Iteration 50, loss = 0.88678525
Iteration 51, loss = 0.88162126
Iteration 52, loss = 0.87684696
Iteration 53, loss = 0.87223583
Iteration 54, loss = 0.86776651
Iteration 55, loss = 0.86349041
Iteration 56, loss = 0.85922698
Iteration 57, loss = 0.85512496
Iteration 58, loss = 0.85116553
Iteration 59, loss = 0.84737031
Iteration 60, loss = 0.84358143
Iteration 61, loss = 0.83999564
Iteration 62, loss = 0.83640949
Iteration 63, loss = 0.83301700
Iteration 64, loss = 0.82964552
Iteration 65, loss = 0.82638099
Iteration 66, loss = 0.82326574
Iteration 67, loss = 0.82015350
Iteration 68, loss = 0.81716899
Iteration 69, loss = 0.81428504
Iteration 70, loss = 0.81141941
Iteration 71, loss = 0.80869742
Iteration 72, loss = 0.80598201
Iteration 73, loss = 0.80336430
Iteration 74, loss = 0.80087402
Iteration 75, loss = 0.79835826
Iteration 76, loss = 0.79596318
Iteration 77, loss = 0.79359610
Iteration 78, loss = 0.79130771
Iteration 79, loss = 0.78904850
Iteration 80, loss = 0.78689516
Iteration 81, loss = 0.78474422
Iteration 82, loss = 0.78267044
Iteration 83, loss = 0.78063287
Iteration 84, loss = 0.77866009
Iteration 85, loss = 0.77673180
Iteration 86, loss = 0.77483860
Iteration 87, loss = 0.77298921
Iteration 88, loss = 0.77118952
Iteration 89, loss = 0.76944148
Iteration 90, loss = 0.76769850
Iteration 91, loss = 0.76604519
Iteration 92, loss = 0.76437642
Iteration 93, loss = 0.76276690
Iteration 94, loss = 0.76119042
Iteration 95, loss = 0.75966026
Iteration 96, loss = 0.75816831
Iteration 97, loss = 0.75667921
Iteration 98, loss = 0.75523563
Iteration 99, loss = 0.75382398
Iteration 100, loss = 0.75243907
Iteration 101, loss = 0.75108847
Iteration 102, loss = 0.74975958
Iteration 103, loss = 0.74845940
Iteration 104, loss = 0.74718776
Iteration 105, loss = 0.74593739
Iteration 106, loss = 0.74471078
Iteration 107, loss = 0.74351041
Iteration 108, loss = 0.74233321
Iteration 109, loss = 0.74117600
Iteration 110, loss = 0.74004049
Iteration 111, loss = 0.73892633
Iteration 112, loss = 0.73783443
Iteration 113, loss = 0.73676234
Iteration 114, loss = 0.73570810
Iteration 115, loss = 0.73467242
Iteration 116, loss = 0.73365495
Iteration 117, loss = 0.73265715
Iteration 118, loss = 0.73167550
Iteration 119, loss = 0.73071046
Iteration 120, loss = 0.72976128
Iteration 121, loss = 0.72883465
Iteration 122, loss = 0.72800566
Iteration 123, loss = 0.72702959
Iteration 124, loss = 0.72613398
Iteration 125, loss = 0.72525951
Iteration 126, loss = 0.72439985
Iteration 127, loss = 0.72356945
Iteration 128, loss = 0.72280888
Iteration 129, loss = 0.72192152
Iteration 130, loss = 0.72111008
Iteration 131, loss = 0.72032067
Iteration 132, loss = 0.71961141
Iteration 133, loss = 0.71878076
Iteration 134, loss = 0.71802051
Iteration 135, loss = 0.71734644
Iteration 136, loss = 0.71654994
Iteration 137, loss = 0.71581966
Iteration 138, loss = 0.71517570
Iteration 139, loss = 0.71441242
Iteration 140, loss = 0.71371890
Iteration 141, loss = 0.71309661
Iteration 142, loss = 0.71236247
Iteration 143, loss = 0.71170826
Iteration 144, loss = 0.71109904
Iteration 145, loss = 0.71040557
Iteration 146, loss = 0.70981095
Iteration 147, loss = 0.70913531
Iteration 148, loss = 0.70856213
Iteration 149, loss = 0.70790300
Iteration 150, loss = 0.70734325
Iteration 151, loss = 0.70670204
Iteration 152, loss = 0.70615344
Iteration 153, loss = 0.70553341
Iteration 154, loss = 0.70499195
Iteration 155, loss = 0.70439087
Iteration 156, loss = 0.70385707
Iteration 157, loss = 0.70327548
Iteration 158, loss = 0.70274898
Iteration 159, loss = 0.70218850
Iteration 160, loss = 0.70166508
Iteration 161, loss = 0.70112181
Iteration 162, loss = 0.70060340
Iteration 163, loss = 0.70008008
Iteration 164, loss = 0.69956373
Iteration 165, loss = 0.69905881
Iteration 166, loss = 0.69854522
Iteration 167, loss = 0.69805513
Iteration 168, loss = 0.69754539
Iteration 169, loss = 0.69707576
Iteration 170, loss = 0.69656506
Iteration 171, loss = 0.69611169
Iteration 172, loss = 0.69560135
Iteration 173, loss = 0.69516415
Iteration 174, loss = 0.69465966
Iteration 175, loss = 0.69418999
Iteration 176, loss = 0.69372957
Iteration 177, loss = 0.69327441
Iteration 178, loss = 0.69286382
Iteration 179, loss = 0.69238002
Iteration 180, loss = 0.69193163
Iteration 181, loss = 0.69149046
Iteration 182, loss = 0.69105289
Iteration 183, loss = 0.69061877
Iteration 184, loss = 0.69018804
Iteration 185, loss = 0.68976020
Iteration 186, loss = 0.68933531
Iteration 187, loss = 0.68891348
Iteration 188, loss = 0.68849410
Iteration 189, loss = 0.68807760
Iteration 190, loss = 0.68766351
Iteration 191, loss = 0.68725192
Iteration 192, loss = 0.68684277
Iteration 193, loss = 0.68643577
Iteration 194, loss = 0.68603111
Iteration 195, loss = 0.68562843
Iteration 196, loss = 0.68522789
Iteration 197, loss = 0.68482924
Iteration 198, loss = 0.68443248
Iteration 199, loss = 0.68403757
Iteration 200, loss = 0.68364429
Iteration 201, loss = 0.68325283
Iteration 202, loss = 0.68286278
Iteration 203, loss = 0.68247440
Iteration 204, loss = 0.68208734
Iteration 205, loss = 0.68170182
Iteration 206, loss = 0.68131749
Iteration 207, loss = 0.68093453
Iteration 208, loss = 0.68055272
Iteration 209, loss = 0.68017208
Iteration 210, loss = 0.67979257
Iteration 211, loss = 0.67941401
Iteration 212, loss = 0.67903658
Iteration 213, loss = 0.67865996
Iteration 214, loss = 0.67828436
Iteration 215, loss = 0.67790943
Iteration 216, loss = 0.67753552
Iteration 217, loss = 0.67716213
Iteration 218, loss = 0.67678960
Iteration 219, loss = 0.67641771
Iteration 220, loss = 0.67604658
Iteration 221, loss = 0.67567600
Iteration 222, loss = 0.67530602
Iteration 223, loss = 0.67493657
Iteration 224, loss = 0.67456752
Iteration 225, loss = 0.67419906
Iteration 226, loss = 0.67383086
Iteration 227, loss = 0.67346315
Iteration 228, loss = 0.67309573
Iteration 229, loss = 0.67274490
Iteration 230, loss = 0.67494853
Iteration 231, loss = 0.67253424
Iteration 232, loss = 0.67178594
Iteration 233, loss = 0.67104646
Iteration 234, loss = 0.67046366
Iteration 235, loss = 0.66991768
Iteration 236, loss = 0.66939020
Iteration 237, loss = 0.66887413
Iteration 238, loss = 0.66837003
Iteration 239, loss = 0.66786730
Iteration 240, loss = 0.66736365
Iteration 241, loss = 0.66685738
Iteration 242, loss = 0.66634834
Iteration 243, loss = 0.66583591
Iteration 244, loss = 0.66532013
Iteration 245, loss = 0.66480118
Iteration 246, loss = 0.66427967
Iteration 247, loss = 0.66375443
Iteration 248, loss = 0.66322586
Iteration 249, loss = 0.66269418
Iteration 250, loss = 0.66217643
Iteration 251, loss = 0.66590451
Iteration 252, loss = 0.66479966
Iteration 253, loss = 0.67297835
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.1
Mean squared error = 0.9
Mean absolute error = 0.3
Median absolute error = 0.0
k_error=4.26363636364
____________________Fin K=3________________________

Pour k=4, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.11895851
Iteration 2, loss = 2.05543151
Iteration 3, loss = 1.96842038
Iteration 4, loss = 1.87240117
Iteration 5, loss = 1.75920655
Iteration 6, loss = 1.60473215
Iteration 7, loss = 1.48007920
Iteration 8, loss = 1.42670749
Iteration 9, loss = 1.38415620
Iteration 10, loss = 1.33320340
Iteration 11, loss = 1.29167877
Iteration 12, loss = 1.25420313
Iteration 13, loss = 1.22011639
Iteration 14, loss = 1.18946346
Iteration 15, loss = 1.16200582
Iteration 16, loss = 1.13810465
Iteration 17, loss = 1.11692557
Iteration 18, loss = 1.09778474
Iteration 19, loss = 1.08011849
Iteration 20, loss = 1.06351212
Iteration 21, loss = 1.04807692
Iteration 22, loss = 1.03363513
Iteration 23, loss = 1.01989317
Iteration 24, loss = 1.00698024
Iteration 25, loss = 0.99478243
Iteration 26, loss = 0.98316429
Iteration 27, loss = 0.97213776
Iteration 28, loss = 0.96175936
Iteration 29, loss = 0.95179256
Iteration 30, loss = 0.94225238
Iteration 31, loss = 0.93320207
Iteration 32, loss = 0.92451981
Iteration 33, loss = 0.91625792
Iteration 34, loss = 0.90817767
Iteration 35, loss = 0.90045261
Iteration 36, loss = 0.89338043
Iteration 37, loss = 0.88627713
Iteration 38, loss = 0.87928530
Iteration 39, loss = 0.87255297
Iteration 40, loss = 0.86614556
Iteration 41, loss = 0.85989100
Iteration 42, loss = 0.85380239
Iteration 43, loss = 0.84793892
Iteration 44, loss = 0.84215461
Iteration 45, loss = 0.83664116
Iteration 46, loss = 0.83166550
Iteration 47, loss = 0.82603020
Iteration 48, loss = 0.82088833
Iteration 49, loss = 0.81589660
Iteration 50, loss = 0.81104057
Iteration 51, loss = 0.80632264
Iteration 52, loss = 0.80174057
Iteration 53, loss = 0.79728115
Iteration 54, loss = 0.79294261
Iteration 55, loss = 0.78872771
Iteration 56, loss = 0.78461882
Iteration 57, loss = 0.78062438
Iteration 58, loss = 0.77673738
Iteration 59, loss = 0.77295257
Iteration 60, loss = 0.76932893
Iteration 61, loss = 0.76569310
Iteration 62, loss = 0.76220476
Iteration 63, loss = 0.75881013
Iteration 64, loss = 0.75550544
Iteration 65, loss = 0.75229132
Iteration 66, loss = 0.74918091
Iteration 67, loss = 0.74613610
Iteration 68, loss = 0.74316933
Iteration 69, loss = 0.74029769
Iteration 70, loss = 0.73748382
Iteration 71, loss = 0.73474352
Iteration 72, loss = 0.73207586
Iteration 73, loss = 0.72955898
Iteration 74, loss = 0.72697070
Iteration 75, loss = 0.72450693
Iteration 76, loss = 0.72209321
Iteration 77, loss = 0.71975480
Iteration 78, loss = 0.71747331
Iteration 79, loss = 0.71528966
Iteration 80, loss = 0.71306312
Iteration 81, loss = 0.71093387
Iteration 82, loss = 0.70885948
Iteration 83, loss = 0.70682327
Iteration 84, loss = 0.70485332
Iteration 85, loss = 0.70296464
Iteration 86, loss = 0.70114180
Iteration 87, loss = 0.69924688
Iteration 88, loss = 0.69740152
Iteration 89, loss = 0.69562730
Iteration 90, loss = 0.69390042
Iteration 91, loss = 0.69222039
Iteration 92, loss = 0.69061439
Iteration 93, loss = 0.68903125
Iteration 94, loss = 0.68741702
Iteration 95, loss = 0.68583740
Iteration 96, loss = 0.68431546
Iteration 97, loss = 0.68282652
Iteration 98, loss = 0.68137306
Iteration 99, loss = 0.67995763
Iteration 100, loss = 0.67860596
Iteration 101, loss = 0.67725918
Iteration 102, loss = 0.67588509
Iteration 103, loss = 0.67454899
Iteration 104, loss = 0.67328793
Iteration 105, loss = 0.67202949
Iteration 106, loss = 0.67079814
Iteration 107, loss = 0.66954294
Iteration 108, loss = 0.66835094
Iteration 109, loss = 0.66717870
Iteration 110, loss = 0.66605863
Iteration 111, loss = 0.66494082
Iteration 112, loss = 0.66382567
Iteration 113, loss = 0.66270886
Iteration 114, loss = 0.66163746
Iteration 115, loss = 0.66061811
Iteration 116, loss = 0.65960080
Iteration 117, loss = 0.65857350
Iteration 118, loss = 0.65755774
Iteration 119, loss = 0.65658966
Iteration 120, loss = 0.65561711
Iteration 121, loss = 0.65466881
Iteration 122, loss = 0.65375459
Iteration 123, loss = 0.65285268
Iteration 124, loss = 0.65194228
Iteration 125, loss = 0.65104315
Iteration 126, loss = 0.65017789
Iteration 127, loss = 0.64931477
Iteration 128, loss = 0.64847825
Iteration 129, loss = 0.64763637
Iteration 130, loss = 0.64683555
Iteration 131, loss = 0.64604094
Iteration 132, loss = 0.64523289
Iteration 133, loss = 0.64444711
Iteration 134, loss = 0.64368138
Iteration 135, loss = 0.64292354
Iteration 136, loss = 0.64217797
Iteration 137, loss = 0.64143926
Iteration 138, loss = 0.64071689
Iteration 139, loss = 0.64000084
Iteration 140, loss = 0.63930249
Iteration 141, loss = 0.63860748
Iteration 142, loss = 0.63792045
Iteration 143, loss = 0.63724839
Iteration 144, loss = 0.63658019
Iteration 145, loss = 0.63593089
Iteration 146, loss = 0.63527719
Iteration 147, loss = 0.63463823
Iteration 148, loss = 0.63401620
Iteration 149, loss = 0.63338850
Iteration 150, loss = 0.63277565
Iteration 151, loss = 0.63217913
Iteration 152, loss = 0.63157501
Iteration 153, loss = 0.63098537
Iteration 154, loss = 0.63040318
Iteration 155, loss = 0.62982870
Iteration 156, loss = 0.62926214
Iteration 157, loss = 0.62870265
Iteration 158, loss = 0.62815439
Iteration 159, loss = 0.62760381
Iteration 160, loss = 0.62706397
Iteration 161, loss = 0.62653083
Iteration 162, loss = 0.62600416
Iteration 163, loss = 0.62548378
Iteration 164, loss = 0.62496969
Iteration 165, loss = 0.62446146
Iteration 166, loss = 0.62395926
Iteration 167, loss = 0.62346290
Iteration 168, loss = 0.62297225
Iteration 169, loss = 0.62254283
Iteration 170, loss = 0.62201728
Iteration 171, loss = 0.62153471
Iteration 172, loss = 0.62106519
Iteration 173, loss = 0.62060091
Iteration 174, loss = 0.62014216
Iteration 175, loss = 0.61973196
Iteration 176, loss = 0.61925054
Iteration 177, loss = 0.61883730
Iteration 178, loss = 0.61836270
Iteration 179, loss = 0.61792344
Iteration 180, loss = 0.61749219
Iteration 181, loss = 0.61706561
Iteration 182, loss = 0.61664324
Iteration 183, loss = 0.61622942
Iteration 184, loss = 0.61584626
Iteration 185, loss = 0.61540736
Iteration 186, loss = 0.61499995
Iteration 187, loss = 0.61459763
Iteration 188, loss = 0.61420359
Iteration 189, loss = 0.61383648
Iteration 190, loss = 0.61342081
Iteration 191, loss = 0.61303389
Iteration 192, loss = 0.61267884
Iteration 193, loss = 0.61227477
Iteration 194, loss = 0.61189599
Iteration 195, loss = 0.61152839
Iteration 196, loss = 0.61117969
Iteration 197, loss = 0.61079327
Iteration 198, loss = 0.61045089
Iteration 199, loss = 0.61007352
Iteration 200, loss = 0.60971506
Iteration 201, loss = 0.60938366
Iteration 202, loss = 0.60901492
Iteration 203, loss = 0.60867017
Iteration 204, loss = 0.60834347
Iteration 205, loss = 0.60798494
Iteration 206, loss = 0.60766284
Iteration 207, loss = 0.60731230
Iteration 208, loss = 0.60698233
Iteration 209, loss = 0.60666653
Iteration 210, loss = 0.60632570
Iteration 211, loss = 0.60601340
Iteration 212, loss = 0.60567867
Iteration 213, loss = 0.60537301
Iteration 214, loss = 0.60504384
Iteration 215, loss = 0.60474264
Iteration 216, loss = 0.60441879
Iteration 217, loss = 0.60412080
Iteration 218, loss = 0.60380305
Iteration 219, loss = 0.60350894
Iteration 220, loss = 0.60319725
Iteration 221, loss = 0.60290512
Iteration 222, loss = 0.60259965
Iteration 223, loss = 0.60231091
Iteration 224, loss = 0.60201074
Iteration 225, loss = 0.60172428
Iteration 226, loss = 0.60142978
Iteration 227, loss = 0.60114634
Iteration 228, loss = 0.60085769
Iteration 229, loss = 0.60057556
Iteration 230, loss = 0.60029279
Iteration 231, loss = 0.60001327
Iteration 232, loss = 0.59973527
Iteration 233, loss = 0.59945756
Iteration 234, loss = 0.59918563
Iteration 235, loss = 0.59890970
Iteration 236, loss = 0.59864251
Iteration 237, loss = 0.59836790
Iteration 238, loss = 0.59810630
Iteration 239, loss = 0.59783314
Iteration 240, loss = 0.59757618
Iteration 241, loss = 0.59730500
Iteration 242, loss = 0.59704209
Iteration 243, loss = 0.59678241
Iteration 244, loss = 0.59652459
Iteration 245, loss = 0.59626825
Iteration 246, loss = 0.59601337
Iteration 247, loss = 0.59575992
Iteration 248, loss = 0.59550799
Iteration 249, loss = 0.59525737
Iteration 250, loss = 0.59500815
Iteration 251, loss = 0.59476027
Iteration 252, loss = 0.59451369
Iteration 253, loss = 0.59426837
Iteration 254, loss = 0.59402429
Iteration 255, loss = 0.59378151
Iteration 256, loss = 0.59353988
Iteration 257, loss = 0.59329941
Iteration 258, loss = 0.59306014
Iteration 259, loss = 0.59282203
Iteration 260, loss = 0.59258495
Iteration 261, loss = 0.59234902
Iteration 262, loss = 0.59211408
Iteration 263, loss = 0.59188029
Iteration 264, loss = 0.59164742
Iteration 265, loss = 0.59141553
Iteration 266, loss = 0.59118477
Iteration 267, loss = 0.59095474
Iteration 268, loss = 0.59072584
Iteration 269, loss = 0.59050644
Iteration 270, loss = 0.59111845
Iteration 271, loss = 0.59041381
Iteration 272, loss = 0.59016500
Iteration 273, loss = 0.58982016
Iteration 274, loss = 0.58947693
Iteration 275, loss = 0.58920224
Iteration 276, loss = 0.58897474
Iteration 277, loss = 0.58875300
Iteration 278, loss = 0.58853117
Iteration 279, loss = 0.58831028
Iteration 280, loss = 0.58809099
Iteration 281, loss = 0.58787298
Iteration 282, loss = 0.58765580
Iteration 283, loss = 0.58743933
Iteration 284, loss = 0.58722360
Iteration 285, loss = 0.58700854
Iteration 286, loss = 0.58679396
Iteration 287, loss = 0.58658012
Iteration 288, loss = 0.58636669
Iteration 289, loss = 0.58615392
Iteration 290, loss = 0.58594156
Iteration 291, loss = 0.58572973
Iteration 292, loss = 0.58551840
Iteration 293, loss = 0.58530746
Iteration 294, loss = 0.58509698
Iteration 295, loss = 0.58488693
Iteration 296, loss = 0.58467725
Iteration 297, loss = 0.58446799
Iteration 298, loss = 0.58425902
Iteration 299, loss = 0.58405040
Iteration 300, loss = 0.58384215
Iteration 301, loss = 0.58363409
Iteration 302, loss = 0.58342639
Iteration 303, loss = 0.58321892
Iteration 304, loss = 0.58301172
Iteration 305, loss = 0.58280465
Iteration 306, loss = 0.58259791
Iteration 307, loss = 0.58239122
Iteration 308, loss = 0.58218480
Iteration 309, loss = 0.58197846
Iteration 310, loss = 0.58177226
Iteration 311, loss = 0.58156621
Iteration 312, loss = 0.58136020
Iteration 313, loss = 0.58115429
Iteration 314, loss = 0.58094845
Iteration 315, loss = 0.58074265
Iteration 316, loss = 0.58053678
Iteration 317, loss = 0.58033110
Iteration 318, loss = 0.58012524
Iteration 319, loss = 0.57991939
Iteration 320, loss = 0.57971357
Iteration 321, loss = 0.57950758
Iteration 322, loss = 0.57930162
Iteration 323, loss = 0.57912912
Iteration 324, loss = 0.58076147
Iteration 325, loss = 0.57913194
Iteration 326, loss = 0.57865668
Iteration 327, loss = 0.57834706
Iteration 328, loss = 0.57813462
Iteration 329, loss = 0.57792927
Iteration 330, loss = 0.57771941
Iteration 331, loss = 0.57750972
Iteration 332, loss = 0.57730147
Iteration 333, loss = 0.57709389
Iteration 334, loss = 0.57688631
Iteration 335, loss = 0.57667856
Iteration 336, loss = 0.57647066
Iteration 337, loss = 0.57626254
Iteration 338, loss = 0.57605424
Iteration 339, loss = 0.57584558
Iteration 340, loss = 0.57563666
Iteration 341, loss = 0.57542743
Iteration 342, loss = 0.57521783
Iteration 343, loss = 0.57500789
Iteration 344, loss = 0.57479764
Iteration 345, loss = 0.57458698
Iteration 346, loss = 0.57437596
Iteration 347, loss = 0.57416461
Iteration 348, loss = 0.57395273
Iteration 349, loss = 0.57374041
Iteration 350, loss = 0.57352773
Iteration 351, loss = 0.57331451
Iteration 352, loss = 0.57310096
Iteration 353, loss = 0.57288720
Iteration 354, loss = 0.57267281
Iteration 355, loss = 0.57245798
Iteration 356, loss = 0.57224284
Iteration 357, loss = 0.57202704
Iteration 358, loss = 0.57181076
Iteration 359, loss = 0.57159406
Iteration 360, loss = 0.57137683
Iteration 361, loss = 0.57115901
Iteration 362, loss = 0.57094077
Iteration 363, loss = 0.57072192
Iteration 364, loss = 0.57050253
Iteration 365, loss = 0.57028261
Iteration 366, loss = 0.57006216
Iteration 367, loss = 0.56984110
Iteration 368, loss = 0.56961955
Iteration 369, loss = 0.56939740
Iteration 370, loss = 0.56917467
Iteration 371, loss = 0.56895138
Iteration 372, loss = 0.56872757
Iteration 373, loss = 0.56850313
Iteration 374, loss = 0.56828396
Iteration 375, loss = 0.57152678
Iteration 376, loss = 0.56913192
Iteration 377, loss = 0.56768285
Iteration 378, loss = 0.56747642
Iteration 379, loss = 0.56723652
Iteration 380, loss = 0.56699715
Iteration 381, loss = 0.56676023
Iteration 382, loss = 0.56652580
Iteration 383, loss = 0.56629138
Iteration 384, loss = 0.56605721
Iteration 385, loss = 0.56582303
Iteration 386, loss = 0.56558892
Iteration 387, loss = 0.56535103
Iteration 388, loss = 0.56545166
Iteration 389, loss = 0.57128817
Iteration 390, loss = 0.56572286
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.4
Mean squared error = 1.8
Mean absolute error = 0.8
Median absolute error = 0.0
k_error=6.06363636364
____________________Fin K=4________________________

Pour k=5, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.10850155
Iteration 2, loss = 2.05836655
Iteration 3, loss = 1.99128118
Iteration 4, loss = 1.91873065
Iteration 5, loss = 1.84049428
Iteration 6, loss = 1.74031205
Iteration 7, loss = 1.61394215
Iteration 8, loss = 1.52888970
Iteration 9, loss = 1.48400318
Iteration 10, loss = 1.43776188
Iteration 11, loss = 1.39219623
Iteration 12, loss = 1.35433466
Iteration 13, loss = 1.31866043
Iteration 14, loss = 1.28662163
Iteration 15, loss = 1.25848461
Iteration 16, loss = 1.23326865
Iteration 17, loss = 1.21058647
Iteration 18, loss = 1.18995549
Iteration 19, loss = 1.17057842
Iteration 20, loss = 1.15240584
Iteration 21, loss = 1.13536657
Iteration 22, loss = 1.11931135
Iteration 23, loss = 1.10413162
Iteration 24, loss = 1.08976820
Iteration 25, loss = 1.07614906
Iteration 26, loss = 1.06321738
Iteration 27, loss = 1.05093798
Iteration 28, loss = 1.03921092
Iteration 29, loss = 1.02800703
Iteration 30, loss = 1.01720987
Iteration 31, loss = 1.00700620
Iteration 32, loss = 0.99725358
Iteration 33, loss = 0.98785667
Iteration 34, loss = 0.97879975
Iteration 35, loss = 0.97010992
Iteration 36, loss = 0.96181042
Iteration 37, loss = 0.95383937
Iteration 38, loss = 0.94625743
Iteration 39, loss = 0.93875337
Iteration 40, loss = 0.93152472
Iteration 41, loss = 0.92454476
Iteration 42, loss = 0.91784732
Iteration 43, loss = 0.91136173
Iteration 44, loss = 0.90509599
Iteration 45, loss = 0.89915601
Iteration 46, loss = 0.89353102
Iteration 47, loss = 0.88754618
Iteration 48, loss = 0.88202753
Iteration 49, loss = 0.87662892
Iteration 50, loss = 0.87150646
Iteration 51, loss = 0.86643142
Iteration 52, loss = 0.86161131
Iteration 53, loss = 0.85684759
Iteration 54, loss = 0.85227823
Iteration 55, loss = 0.84787592
Iteration 56, loss = 0.84353138
Iteration 57, loss = 0.83940054
Iteration 58, loss = 0.83530817
Iteration 59, loss = 0.83136024
Iteration 60, loss = 0.82754689
Iteration 61, loss = 0.82389102
Iteration 62, loss = 0.82028618
Iteration 63, loss = 0.81680670
Iteration 64, loss = 0.81342177
Iteration 65, loss = 0.81013548
Iteration 66, loss = 0.80690522
Iteration 67, loss = 0.80384276
Iteration 68, loss = 0.80078401
Iteration 69, loss = 0.79786241
Iteration 70, loss = 0.79499822
Iteration 71, loss = 0.79222018
Iteration 72, loss = 0.78949388
Iteration 73, loss = 0.78687187
Iteration 74, loss = 0.78431080
Iteration 75, loss = 0.78180407
Iteration 76, loss = 0.77936520
Iteration 77, loss = 0.77700036
Iteration 78, loss = 0.77469195
Iteration 79, loss = 0.77242428
Iteration 80, loss = 0.77023434
Iteration 81, loss = 0.76808287
Iteration 82, loss = 0.76599073
Iteration 83, loss = 0.76394710
Iteration 84, loss = 0.76195874
Iteration 85, loss = 0.76001801
Iteration 86, loss = 0.75811912
Iteration 87, loss = 0.75626519
Iteration 88, loss = 0.75446175
Iteration 89, loss = 0.75269405
Iteration 90, loss = 0.75096417
Iteration 91, loss = 0.74927877
Iteration 92, loss = 0.74762451
Iteration 93, loss = 0.74600617
Iteration 94, loss = 0.74442985
Iteration 95, loss = 0.74287985
Iteration 96, loss = 0.74136186
Iteration 97, loss = 0.73988239
Iteration 98, loss = 0.73842758
Iteration 99, loss = 0.73700228
Iteration 100, loss = 0.73560533
Iteration 101, loss = 0.73424045
Iteration 102, loss = 0.73290026
Iteration 103, loss = 0.73158441
Iteration 104, loss = 0.73029395
Iteration 105, loss = 0.72902821
Iteration 106, loss = 0.72778990
Iteration 107, loss = 0.72657059
Iteration 108, loss = 0.72537470
Iteration 109, loss = 0.72420099
Iteration 110, loss = 0.72304824
Iteration 111, loss = 0.72191827
Iteration 112, loss = 0.72080626
Iteration 113, loss = 0.71971356
Iteration 114, loss = 0.71863959
Iteration 115, loss = 0.71758390
Iteration 116, loss = 0.71654448
Iteration 117, loss = 0.71552164
Iteration 118, loss = 0.71451552
Iteration 119, loss = 0.71352622
Iteration 120, loss = 0.71255257
Iteration 121, loss = 0.71159477
Iteration 122, loss = 0.71065214
Iteration 123, loss = 0.70972395
Iteration 124, loss = 0.70881004
Iteration 125, loss = 0.70791014
Iteration 126, loss = 0.70702367
Iteration 127, loss = 0.70615034
Iteration 128, loss = 0.70529004
Iteration 129, loss = 0.70444209
Iteration 130, loss = 0.70360604
Iteration 131, loss = 0.70278173
Iteration 132, loss = 0.70196908
Iteration 133, loss = 0.70116720
Iteration 134, loss = 0.70037620
Iteration 135, loss = 0.69959588
Iteration 136, loss = 0.69882569
Iteration 137, loss = 0.69806537
Iteration 138, loss = 0.69731492
Iteration 139, loss = 0.69657377
Iteration 140, loss = 0.69584173
Iteration 141, loss = 0.69511853
Iteration 142, loss = 0.69440397
Iteration 143, loss = 0.69369790
Iteration 144, loss = 0.69300010
Iteration 145, loss = 0.69231011
Iteration 146, loss = 0.69162786
Iteration 147, loss = 0.69095338
Iteration 148, loss = 0.69028591
Iteration 149, loss = 0.68962574
Iteration 150, loss = 0.68897240
Iteration 151, loss = 0.68832591
Iteration 152, loss = 0.68768582
Iteration 153, loss = 0.68705211
Iteration 154, loss = 0.68642460
Iteration 155, loss = 0.68580516
Iteration 156, loss = 0.68604551
Iteration 157, loss = 0.68477750
Iteration 158, loss = 0.68408110
Iteration 159, loss = 0.68345475
Iteration 160, loss = 0.68284963
Iteration 161, loss = 0.68225739
Iteration 162, loss = 0.68167324
Iteration 163, loss = 0.68109448
Iteration 164, loss = 0.68052027
Iteration 165, loss = 0.67995089
Iteration 166, loss = 0.67938627
Iteration 167, loss = 0.67882600
Iteration 168, loss = 0.67826992
Iteration 169, loss = 0.67771789
Iteration 170, loss = 0.67716979
Iteration 171, loss = 0.67662550
Iteration 172, loss = 0.67608492
Iteration 173, loss = 0.67554794
Iteration 174, loss = 0.67501445
Iteration 175, loss = 0.67450966
Iteration 176, loss = 0.67503388
Iteration 177, loss = 0.67370201
Iteration 178, loss = 0.67302536
Iteration 179, loss = 0.67248028
Iteration 180, loss = 0.67195685
Iteration 181, loss = 0.67144399
Iteration 182, loss = 0.67093534
Iteration 183, loss = 0.67042908
Iteration 184, loss = 0.66992521
Iteration 185, loss = 0.66942398
Iteration 186, loss = 0.66892540
Iteration 187, loss = 0.66842966
Iteration 188, loss = 0.66793644
Iteration 189, loss = 0.66744549
Iteration 190, loss = 0.66695676
Iteration 191, loss = 0.66647017
Iteration 192, loss = 0.66598570
Iteration 193, loss = 0.66550329
Iteration 194, loss = 0.66502285
Iteration 195, loss = 0.66457120
Iteration 196, loss = 0.66543854
Iteration 197, loss = 0.66392710
Iteration 198, loss = 0.66304336
Iteration 199, loss = 0.66223359
Iteration 200, loss = 0.66158323
Iteration 201, loss = 0.66095591
Iteration 202, loss = 0.66042212
Iteration 203, loss = 0.66169291
Iteration 204, loss = 0.66020793
Iteration 205, loss = 0.65906833
Iteration 206, loss = 0.65824789
Iteration 207, loss = 0.65757491
Iteration 208, loss = 0.65694746
Iteration 209, loss = 0.65634085
Iteration 210, loss = 0.65574077
Iteration 211, loss = 0.65514202
Iteration 212, loss = 0.65454349
Iteration 213, loss = 0.65394432
Iteration 214, loss = 0.65334411
Iteration 215, loss = 0.65274246
Iteration 216, loss = 0.65213940
Iteration 217, loss = 0.65153471
Iteration 218, loss = 0.65101676
Iteration 219, loss = 0.65278152
Iteration 220, loss = 0.65156273
Iteration 221, loss = 0.65576981
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.2
Mean squared error = 1.3
Mean absolute error = 0.5
Median absolute error = 0.0
k_error=7.36363636364
____________________Fin K=5________________________

Pour k=6, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.12986139
Iteration 2, loss = 2.06700122
Iteration 3, loss = 1.98097875
Iteration 4, loss = 1.88646679
Iteration 5, loss = 1.77884550
Iteration 6, loss = 1.63793303
Iteration 7, loss = 1.51422207
Iteration 8, loss = 1.45126192
Iteration 9, loss = 1.40375230
Iteration 10, loss = 1.34960798
Iteration 11, loss = 1.30561336
Iteration 12, loss = 1.26656647
Iteration 13, loss = 1.23283757
Iteration 14, loss = 1.20426530
Iteration 15, loss = 1.17958772
Iteration 16, loss = 1.15839993
Iteration 17, loss = 1.13936593
Iteration 18, loss = 1.12204353
Iteration 19, loss = 1.10608555
Iteration 20, loss = 1.09123170
Iteration 21, loss = 1.07739492
Iteration 22, loss = 1.06436345
Iteration 23, loss = 1.05206762
Iteration 24, loss = 1.04047350
Iteration 25, loss = 1.02948083
Iteration 26, loss = 1.01899997
Iteration 27, loss = 1.00902710
Iteration 28, loss = 0.99948624
Iteration 29, loss = 0.99036766
Iteration 30, loss = 0.98170443
Iteration 31, loss = 0.97336401
Iteration 32, loss = 0.96537537
Iteration 33, loss = 0.95766542
Iteration 34, loss = 0.95012196
Iteration 35, loss = 0.94292013
Iteration 36, loss = 0.93591958
Iteration 37, loss = 0.92932833
Iteration 38, loss = 0.92263052
Iteration 39, loss = 0.91635277
Iteration 40, loss = 0.91016581
Iteration 41, loss = 0.90407060
Iteration 42, loss = 0.89812583
Iteration 43, loss = 0.89245261
Iteration 44, loss = 0.88690790
Iteration 45, loss = 0.88147152
Iteration 46, loss = 0.87618770
Iteration 47, loss = 0.87137730
Iteration 48, loss = 0.86606884
Iteration 49, loss = 0.86114953
Iteration 50, loss = 0.85635603
Iteration 51, loss = 0.85174160
Iteration 52, loss = 0.84726745
Iteration 53, loss = 0.84279261
Iteration 54, loss = 0.83851875
Iteration 55, loss = 0.83439298
Iteration 56, loss = 0.83026198
Iteration 57, loss = 0.82628942
Iteration 58, loss = 0.82242193
Iteration 59, loss = 0.81867495
Iteration 60, loss = 0.81508737
Iteration 61, loss = 0.81148511
Iteration 62, loss = 0.80802112
Iteration 63, loss = 0.80471058
Iteration 64, loss = 0.80137288
Iteration 65, loss = 0.79815981
Iteration 66, loss = 0.79503009
Iteration 67, loss = 0.79198748
Iteration 68, loss = 0.78907252
Iteration 69, loss = 0.78620451
Iteration 70, loss = 0.78334810
Iteration 71, loss = 0.78059883
Iteration 72, loss = 0.77792497
Iteration 73, loss = 0.77539936
Iteration 74, loss = 0.77278876
Iteration 75, loss = 0.77031145
Iteration 76, loss = 0.76791740
Iteration 77, loss = 0.76563599
Iteration 78, loss = 0.76337730
Iteration 79, loss = 0.76108557
Iteration 80, loss = 0.75886761
Iteration 81, loss = 0.75672392
Iteration 82, loss = 0.75463989
Iteration 83, loss = 0.75260532
Iteration 84, loss = 0.75062247
Iteration 85, loss = 0.74874783
Iteration 86, loss = 0.74686406
Iteration 87, loss = 0.74502149
Iteration 88, loss = 0.74315209
Iteration 89, loss = 0.74138651
Iteration 90, loss = 0.73965361
Iteration 91, loss = 0.73803942
Iteration 92, loss = 0.73637378
Iteration 93, loss = 0.73474744
Iteration 94, loss = 0.73312439
Iteration 95, loss = 0.73157110
Iteration 96, loss = 0.73006286
Iteration 97, loss = 0.72863956
Iteration 98, loss = 0.72717962
Iteration 99, loss = 0.72573900
Iteration 100, loss = 0.72432275
Iteration 101, loss = 0.72295460
Iteration 102, loss = 0.72161255
Iteration 103, loss = 0.72030540
Iteration 104, loss = 0.71909326
Iteration 105, loss = 0.71781002
Iteration 106, loss = 0.71656743
Iteration 107, loss = 0.71537776
Iteration 108, loss = 0.71416569
Iteration 109, loss = 0.71297051
Iteration 110, loss = 0.71182866
Iteration 111, loss = 0.71074763
Iteration 112, loss = 0.70964160
Iteration 113, loss = 0.70854237
Iteration 114, loss = 0.70746172
Iteration 115, loss = 0.70642092
Iteration 116, loss = 0.70543206
Iteration 117, loss = 0.70442692
Iteration 118, loss = 0.70342044
Iteration 119, loss = 0.70243393
Iteration 120, loss = 0.70148367
Iteration 121, loss = 0.70056900
Iteration 122, loss = 0.69965657
Iteration 123, loss = 0.69873871
Iteration 124, loss = 0.69784003
Iteration 125, loss = 0.69697742
Iteration 126, loss = 0.69612939
Iteration 127, loss = 0.69527062
Iteration 128, loss = 0.69443270
Iteration 129, loss = 0.69362033
Iteration 130, loss = 0.69280496
Iteration 131, loss = 0.69203358
Iteration 132, loss = 0.69126010
Iteration 133, loss = 0.69047827
Iteration 134, loss = 0.68971884
Iteration 135, loss = 0.68898179
Iteration 136, loss = 0.68824822
Iteration 137, loss = 0.68752969
Iteration 138, loss = 0.68681707
Iteration 139, loss = 0.68612473
Iteration 140, loss = 0.68544928
Iteration 141, loss = 0.68475481
Iteration 142, loss = 0.68409189
Iteration 143, loss = 0.68343143
Iteration 144, loss = 0.68278938
Iteration 145, loss = 0.68214300
Iteration 146, loss = 0.68151585
Iteration 147, loss = 0.68089938
Iteration 148, loss = 0.68028247
Iteration 149, loss = 0.67968558
Iteration 150, loss = 0.67908716
Iteration 151, loss = 0.67850361
Iteration 152, loss = 0.67793212
Iteration 153, loss = 0.67735707
Iteration 154, loss = 0.67679522
Iteration 155, loss = 0.67624087
Iteration 156, loss = 0.67569419
Iteration 157, loss = 0.67515538
Iteration 158, loss = 0.67462368
Iteration 159, loss = 0.67409874
Iteration 160, loss = 0.67358087
Iteration 161, loss = 0.67306967
Iteration 162, loss = 0.67256522
Iteration 163, loss = 0.67206703
Iteration 164, loss = 0.67157530
Iteration 165, loss = 0.67108976
Iteration 166, loss = 0.67061018
Iteration 167, loss = 0.67013658
Iteration 168, loss = 0.66966887
Iteration 169, loss = 0.66920679
Iteration 170, loss = 0.66875031
Iteration 171, loss = 0.66829951
Iteration 172, loss = 0.66785390
Iteration 173, loss = 0.66741365
Iteration 174, loss = 0.66697866
Iteration 175, loss = 0.66654876
Iteration 176, loss = 0.66612379
Iteration 177, loss = 0.66570377
Iteration 178, loss = 0.66528850
Iteration 179, loss = 0.66487803
Iteration 180, loss = 0.66447218
Iteration 181, loss = 0.66407082
Iteration 182, loss = 0.66367392
Iteration 183, loss = 0.66328149
Iteration 184, loss = 0.66289327
Iteration 185, loss = 0.66250925
Iteration 186, loss = 0.66212943
Iteration 187, loss = 0.66175362
Iteration 188, loss = 0.66138180
Iteration 189, loss = 0.66101382
Iteration 190, loss = 0.66064972
Iteration 191, loss = 0.66028930
Iteration 192, loss = 0.65993263
Iteration 193, loss = 0.65957949
Iteration 194, loss = 0.65922993
Iteration 195, loss = 0.65888381
Iteration 196, loss = 0.65854108
Iteration 197, loss = 0.65820177
Iteration 198, loss = 0.65786566
Iteration 199, loss = 0.65753272
Iteration 200, loss = 0.65720291
Iteration 201, loss = 0.65687623
Iteration 202, loss = 0.65655252
Iteration 203, loss = 0.65623172
Iteration 204, loss = 0.65591378
Iteration 205, loss = 0.65559872
Iteration 206, loss = 0.65528639
Iteration 207, loss = 0.65497674
Iteration 208, loss = 0.65466972
Iteration 209, loss = 0.65436531
Iteration 210, loss = 0.65406649
Iteration 211, loss = 0.65379812
Iteration 212, loss = 0.65347468
Iteration 213, loss = 0.65317992
Iteration 214, loss = 0.65289124
Iteration 215, loss = 0.65263072
Iteration 216, loss = 0.65231759
Iteration 217, loss = 0.65203616
Iteration 218, loss = 0.65177837
Iteration 219, loss = 0.65147483
Iteration 220, loss = 0.65120281
Iteration 221, loss = 0.65094725
Iteration 222, loss = 0.65064919
Iteration 223, loss = 0.65039047
Iteration 224, loss = 0.65013352
Iteration 225, loss = 0.64984439
Iteration 226, loss = 0.64959955
Iteration 227, loss = 0.64931897
Iteration 228, loss = 0.64907594
Iteration 229, loss = 0.64880001
Iteration 230, loss = 0.64855831
Iteration 231, loss = 0.64828832
Iteration 232, loss = 0.64804669
Iteration 233, loss = 0.64778283
Iteration 234, loss = 0.64754007
Iteration 235, loss = 0.64728428
Iteration 236, loss = 0.64703972
Iteration 237, loss = 0.64679047
Iteration 238, loss = 0.64654396
Iteration 239, loss = 0.64630172
Iteration 240, loss = 0.64605277
Iteration 241, loss = 0.64585090
Iteration 242, loss = 0.64617907
Iteration 243, loss = 0.64541503
Iteration 244, loss = 0.64515123
Iteration 245, loss = 0.64489743
Iteration 246, loss = 0.64464054
Iteration 247, loss = 0.64440734
Iteration 248, loss = 0.64417600
Iteration 249, loss = 0.64392718
Iteration 250, loss = 0.64371334
Iteration 251, loss = 0.64346131
Iteration 252, loss = 0.64322775
Iteration 253, loss = 0.64299526
Iteration 254, loss = 0.64276442
Iteration 255, loss = 0.64253440
Iteration 256, loss = 0.64230516
Iteration 257, loss = 0.64207638
Iteration 258, loss = 0.64184808
Iteration 259, loss = 0.64162014
Iteration 260, loss = 0.64139252
Iteration 261, loss = 0.64116518
Iteration 262, loss = 0.64093805
Iteration 263, loss = 0.64071107
Iteration 264, loss = 0.64048428
Iteration 265, loss = 0.64025748
Iteration 266, loss = 0.64003077
Iteration 267, loss = 0.63980400
Iteration 268, loss = 0.63957725
Iteration 269, loss = 0.63935027
Iteration 270, loss = 0.63912332
Iteration 271, loss = 0.63889600
Iteration 272, loss = 0.63866863
Iteration 273, loss = 0.63844085
Iteration 274, loss = 0.63821283
Iteration 275, loss = 0.63798447
Iteration 276, loss = 0.63775568
Iteration 277, loss = 0.63752870
Iteration 278, loss = 0.63808588
Iteration 279, loss = 0.63714798
Iteration 280, loss = 0.63689597
Iteration 281, loss = 0.63664499
Iteration 282, loss = 0.63640898
Iteration 283, loss = 0.63617600
Iteration 284, loss = 0.63594347
Iteration 285, loss = 0.63571030
Iteration 286, loss = 0.63547637
Iteration 287, loss = 0.63524182
Iteration 288, loss = 0.63500646
Iteration 289, loss = 0.63477033
Iteration 290, loss = 0.63453359
Iteration 291, loss = 0.63429585
Iteration 292, loss = 0.63405735
Iteration 293, loss = 0.63381794
Iteration 294, loss = 0.63357761
Iteration 295, loss = 0.63333645
Iteration 296, loss = 0.63309421
Iteration 297, loss = 0.63285102
Iteration 298, loss = 0.63260692
Iteration 299, loss = 0.63236169
Iteration 300, loss = 0.63211547
Iteration 301, loss = 0.63186823
Iteration 302, loss = 0.63161986
Iteration 303, loss = 0.63137045
Iteration 304, loss = 0.63111998
Iteration 305, loss = 0.63088364
Iteration 306, loss = 0.63172098
Iteration 307, loss = 0.63053235
Iteration 308, loss = 0.63016498
Iteration 309, loss = 0.62989626
Iteration 310, loss = 0.62963542
Iteration 311, loss = 0.62937687
Iteration 312, loss = 0.62911786
Iteration 313, loss = 0.62885749
Iteration 314, loss = 0.62859627
Iteration 315, loss = 0.62833394
Iteration 316, loss = 0.62806693
Iteration 317, loss = 0.62814599
Iteration 318, loss = 0.62877575
Iteration 319, loss = 0.62673440
Iteration 320, loss = 0.62632139
Iteration 321, loss = 0.62579177
Iteration 322, loss = 0.62533686
Iteration 323, loss = 0.62488846
Iteration 324, loss = 0.62445363
Iteration 325, loss = 0.62402210
Iteration 326, loss = 0.62359037
Iteration 327, loss = 0.62315580
Iteration 328, loss = 0.62271737
Iteration 329, loss = 0.62227545
Iteration 330, loss = 0.62182862
Iteration 331, loss = 0.62137708
Iteration 332, loss = 0.62092089
Iteration 333, loss = 0.62046048
Iteration 334, loss = 0.61999664
Iteration 335, loss = 0.61952801
Iteration 336, loss = 0.61905527
Iteration 337, loss = 0.61857899
Iteration 338, loss = 0.61809937
Iteration 339, loss = 0.61761876
Iteration 340, loss = 0.61732090
Iteration 341, loss = 0.61973381
Iteration 342, loss = 0.62029699
Iteration 343, loss = 0.62877461
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.2
Mean squared error = 1.8
Mean absolute error = 0.6
Median absolute error = 0.0
k_error=9.16363636364
____________________Fin K=6________________________

Pour k=7, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.12485234
Iteration 2, loss = 2.07663801
Iteration 3, loss = 2.01852666
Iteration 4, loss = 1.95072989
Iteration 5, loss = 1.88218212
Iteration 6, loss = 1.80838295
Iteration 7, loss = 1.72078078
Iteration 8, loss = 1.61058451
Iteration 9, loss = 1.53537656
Iteration 10, loss = 1.49174252
Iteration 11, loss = 1.44611768
Iteration 12, loss = 1.40444162
Iteration 13, loss = 1.36573078
Iteration 14, loss = 1.32989123
Iteration 15, loss = 1.29681222
Iteration 16, loss = 1.26775295
Iteration 17, loss = 1.24161607
Iteration 18, loss = 1.21824968
Iteration 19, loss = 1.19710700
Iteration 20, loss = 1.17725300
Iteration 21, loss = 1.15875379
Iteration 22, loss = 1.14137439
Iteration 23, loss = 1.12498974
Iteration 24, loss = 1.10975453
Iteration 25, loss = 1.09544329
Iteration 26, loss = 1.08177916
Iteration 27, loss = 1.06862398
Iteration 28, loss = 1.05607718
Iteration 29, loss = 1.04386906
Iteration 30, loss = 1.03216296
Iteration 31, loss = 1.02086256
Iteration 32, loss = 1.01053680
Iteration 33, loss = 1.00044780
Iteration 34, loss = 0.99064134
Iteration 35, loss = 0.98147789
Iteration 36, loss = 0.97259500
Iteration 37, loss = 0.96397737
Iteration 38, loss = 0.95562089
Iteration 39, loss = 0.94761100
Iteration 40, loss = 0.93983571
Iteration 41, loss = 0.93229606
Iteration 42, loss = 0.92500486
Iteration 43, loss = 0.91807759
Iteration 44, loss = 0.91115493
Iteration 45, loss = 0.90460756
Iteration 46, loss = 0.89815399
Iteration 47, loss = 0.89196366
Iteration 48, loss = 0.88594522
Iteration 49, loss = 0.88004457
Iteration 50, loss = 0.87448817
Iteration 51, loss = 0.86891743
Iteration 52, loss = 0.86358609
Iteration 53, loss = 0.85845082
Iteration 54, loss = 0.85357335
Iteration 55, loss = 0.84865893
Iteration 56, loss = 0.84396431
Iteration 57, loss = 0.83942472
Iteration 58, loss = 0.83502697
Iteration 59, loss = 0.83078750
Iteration 60, loss = 0.82673967
Iteration 61, loss = 0.82266129
Iteration 62, loss = 0.81878160
Iteration 63, loss = 0.81501352
Iteration 64, loss = 0.81136013
Iteration 65, loss = 0.80782535
Iteration 66, loss = 0.80452160
Iteration 67, loss = 0.80107548
Iteration 68, loss = 0.79782761
Iteration 69, loss = 0.79468281
Iteration 70, loss = 0.79165015
Iteration 71, loss = 0.78874784
Iteration 72, loss = 0.78579699
Iteration 73, loss = 0.78297492
Iteration 74, loss = 0.78023982
Iteration 75, loss = 0.77761428
Iteration 76, loss = 0.77512337
Iteration 77, loss = 0.77252321
Iteration 78, loss = 0.77007066
Iteration 79, loss = 0.76772000
Iteration 80, loss = 0.76547528
Iteration 81, loss = 0.76315302
Iteration 82, loss = 0.76103217
Iteration 83, loss = 0.75882355
Iteration 84, loss = 0.75671382
Iteration 85, loss = 0.75466397
Iteration 86, loss = 0.75270807
Iteration 87, loss = 0.75081098
Iteration 88, loss = 0.74889529
Iteration 89, loss = 0.74708283
Iteration 90, loss = 0.74522098
Iteration 91, loss = 0.74344592
Iteration 92, loss = 0.74179418
Iteration 93, loss = 0.74004999
Iteration 94, loss = 0.73839909
Iteration 95, loss = 0.73684517
Iteration 96, loss = 0.73522665
Iteration 97, loss = 0.73373555
Iteration 98, loss = 0.73218177
Iteration 99, loss = 0.73069749
Iteration 100, loss = 0.72932626
Iteration 101, loss = 0.72785903
Iteration 102, loss = 0.72652738
Iteration 103, loss = 0.72512794
Iteration 104, loss = 0.72384425
Iteration 105, loss = 0.72250604
Iteration 106, loss = 0.72124133
Iteration 107, loss = 0.72003172
Iteration 108, loss = 0.71878532
Iteration 109, loss = 0.71760296
Iteration 110, loss = 0.71640872
Iteration 111, loss = 0.71528084
Iteration 112, loss = 0.71413237
Iteration 113, loss = 0.71303762
Iteration 114, loss = 0.71193191
Iteration 115, loss = 0.71088063
Iteration 116, loss = 0.70982346
Iteration 117, loss = 0.70879733
Iteration 118, loss = 0.70777923
Iteration 119, loss = 0.70678887
Iteration 120, loss = 0.70581184
Iteration 121, loss = 0.70484935
Iteration 122, loss = 0.70391212
Iteration 123, loss = 0.70297558
Iteration 124, loss = 0.70207222
Iteration 125, loss = 0.70116305
Iteration 126, loss = 0.70029785
Iteration 127, loss = 0.69940930
Iteration 128, loss = 0.69857823
Iteration 129, loss = 0.69771627
Iteration 130, loss = 0.69688637
Iteration 131, loss = 0.69609759
Iteration 132, loss = 0.69527506
Iteration 133, loss = 0.69448415
Iteration 134, loss = 0.69370690
Iteration 135, loss = 0.69294320
Iteration 136, loss = 0.69220666
Iteration 137, loss = 0.69144759
Iteration 138, loss = 0.69071427
Iteration 139, loss = 0.68999221
Iteration 140, loss = 0.68928067
Iteration 141, loss = 0.68857940
Iteration 142, loss = 0.68788762
Iteration 143, loss = 0.68720540
Iteration 144, loss = 0.68653242
Iteration 145, loss = 0.68586827
Iteration 146, loss = 0.68521299
Iteration 147, loss = 0.68456613
Iteration 148, loss = 0.68392764
Iteration 149, loss = 0.68329726
Iteration 150, loss = 0.68267468
Iteration 151, loss = 0.68206004
Iteration 152, loss = 0.68145265
Iteration 153, loss = 0.68085289
Iteration 154, loss = 0.68026013
Iteration 155, loss = 0.67967446
Iteration 156, loss = 0.67909589
Iteration 157, loss = 0.67852376
Iteration 158, loss = 0.67795834
Iteration 159, loss = 0.67739934
Iteration 160, loss = 0.67684643
Iteration 161, loss = 0.67629963
Iteration 162, loss = 0.67575894
Iteration 163, loss = 0.67522385
Iteration 164, loss = 0.67471338
Iteration 165, loss = 0.67601982
Iteration 166, loss = 0.67402484
Iteration 167, loss = 0.67340517
Iteration 168, loss = 0.67277995
Iteration 169, loss = 0.67222227
Iteration 170, loss = 0.67170688
Iteration 171, loss = 0.67120831
Iteration 172, loss = 0.67071793
Iteration 173, loss = 0.67023281
Iteration 174, loss = 0.66975234
Iteration 175, loss = 0.66926486
Iteration 176, loss = 0.66866140
Iteration 177, loss = 0.66809389
Iteration 178, loss = 0.66755200
Iteration 179, loss = 0.66701802
Iteration 180, loss = 0.66649129
Iteration 181, loss = 0.66597131
Iteration 182, loss = 0.66545796
Iteration 183, loss = 0.66495000
Iteration 184, loss = 0.66444661
Iteration 185, loss = 0.66394724
Iteration 186, loss = 0.66345148
Iteration 187, loss = 0.66297669
Iteration 188, loss = 0.66518321
Iteration 189, loss = 0.66249285
Iteration 190, loss = 0.66187422
Iteration 191, loss = 0.66122854
Iteration 192, loss = 0.66070921
Iteration 193, loss = 0.66022313
Iteration 194, loss = 0.65974511
Iteration 195, loss = 0.65926882
Iteration 196, loss = 0.65879420
Iteration 197, loss = 0.65832182
Iteration 198, loss = 0.65785160
Iteration 199, loss = 0.65738307
Iteration 200, loss = 0.65691583
Iteration 201, loss = 0.65644968
Iteration 202, loss = 0.65598454
Iteration 203, loss = 0.65552055
Iteration 204, loss = 0.65505726
Iteration 205, loss = 0.65459470
Iteration 206, loss = 0.65413294
Iteration 207, loss = 0.65367181
Iteration 208, loss = 0.65321151
Iteration 209, loss = 0.65275165
Iteration 210, loss = 0.65229245
Iteration 211, loss = 0.65183362
Iteration 212, loss = 0.65137541
Iteration 213, loss = 0.65091750
Iteration 214, loss = 0.65046008
Iteration 215, loss = 0.65000292
Iteration 216, loss = 0.64954623
Iteration 217, loss = 0.64908975
Iteration 218, loss = 0.64863366
Iteration 219, loss = 0.64821121
Iteration 220, loss = 0.65193035
Iteration 221, loss = 0.64840969
Iteration 222, loss = 0.64714940
Iteration 223, loss = 0.64655884
Iteration 224, loss = 0.64605979
Iteration 225, loss = 0.64559763
Iteration 226, loss = 0.64513777
Iteration 227, loss = 0.64467956
Iteration 228, loss = 0.64422337
Iteration 229, loss = 0.64376873
Iteration 230, loss = 0.64331493
Iteration 231, loss = 0.64286161
Iteration 232, loss = 0.64240879
Iteration 233, loss = 0.64195630
Iteration 234, loss = 0.64150395
Iteration 235, loss = 0.64105192
Iteration 236, loss = 0.64060017
Iteration 237, loss = 0.64014867
Iteration 238, loss = 0.63969741
Iteration 239, loss = 0.63924648
Iteration 240, loss = 0.63879576
Iteration 241, loss = 0.63834536
Iteration 242, loss = 0.63789524
Iteration 243, loss = 0.63744541
Iteration 244, loss = 0.63699600
Iteration 245, loss = 0.63654687
Iteration 246, loss = 0.63609818
Iteration 247, loss = 0.63564985
Iteration 248, loss = 0.63520203
Iteration 249, loss = 0.63475472
Iteration 250, loss = 0.63430781
Iteration 251, loss = 0.63386152
Iteration 252, loss = 0.63341568
Iteration 253, loss = 0.63297044
Iteration 254, loss = 0.63252584
Iteration 255, loss = 0.63208175
Iteration 256, loss = 0.63163847
Iteration 257, loss = 0.63124426
Iteration 258, loss = 0.63755139
Iteration 259, loss = 0.63333238
Iteration 260, loss = 0.63193905
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.3
Mean squared error = 1.1
Mean absolute error = 0.5
Median absolute error = 0.0
k_error=10.2636363636
____________________Fin K=7________________________

Pour k=8, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.11395632
Iteration 2, loss = 2.05974081
Iteration 3, loss = 1.98472203
Iteration 4, loss = 1.90178529
Iteration 5, loss = 1.80810995
Iteration 6, loss = 1.68040174
Iteration 7, loss = 1.56162919
Iteration 8, loss = 1.49340261
Iteration 9, loss = 1.44423550
Iteration 10, loss = 1.39042543
Iteration 11, loss = 1.34557929
Iteration 12, loss = 1.30687597
Iteration 13, loss = 1.27295323
Iteration 14, loss = 1.24368305
Iteration 15, loss = 1.21859319
Iteration 16, loss = 1.19729449
Iteration 17, loss = 1.17808227
Iteration 18, loss = 1.16054672
Iteration 19, loss = 1.14417713
Iteration 20, loss = 1.12886870
Iteration 21, loss = 1.11456912
Iteration 22, loss = 1.10113062
Iteration 23, loss = 1.08839763
Iteration 24, loss = 1.07630717
Iteration 25, loss = 1.06480820
Iteration 26, loss = 1.05381803
Iteration 27, loss = 1.04337160
Iteration 28, loss = 1.03331755
Iteration 29, loss = 1.02358571
Iteration 30, loss = 1.01445833
Iteration 31, loss = 1.00590057
Iteration 32, loss = 0.99692992
Iteration 33, loss = 0.98885036
Iteration 34, loss = 0.98097245
Iteration 35, loss = 0.97349609
Iteration 36, loss = 0.96612459
Iteration 37, loss = 0.95876531
Iteration 38, loss = 0.95177650
Iteration 39, loss = 0.94514975
Iteration 40, loss = 0.93861762
Iteration 41, loss = 0.93233427
Iteration 42, loss = 0.92624803
Iteration 43, loss = 0.92017059
Iteration 44, loss = 0.91458906
Iteration 45, loss = 0.90872293
Iteration 46, loss = 0.90327575
Iteration 47, loss = 0.89810897
Iteration 48, loss = 0.89381859
Iteration 49, loss = 0.88796524
Iteration 50, loss = 0.88301219
Iteration 51, loss = 0.87819356
Iteration 52, loss = 0.87355003
Iteration 53, loss = 0.86911940
Iteration 54, loss = 0.86486624
Iteration 55, loss = 0.86086547
Iteration 56, loss = 0.85651467
Iteration 57, loss = 0.85254281
Iteration 58, loss = 0.84867719
Iteration 59, loss = 0.84490037
Iteration 60, loss = 0.84154549
Iteration 61, loss = 0.83776438
Iteration 62, loss = 0.83426416
Iteration 63, loss = 0.83089053
Iteration 64, loss = 0.82765532
Iteration 65, loss = 0.82447673
Iteration 66, loss = 0.82138007
Iteration 67, loss = 0.81835494
Iteration 68, loss = 0.81544475
Iteration 69, loss = 0.81258841
Iteration 70, loss = 0.80982893
Iteration 71, loss = 0.80712071
Iteration 72, loss = 0.80448779
Iteration 73, loss = 0.80191678
Iteration 74, loss = 0.79943354
Iteration 75, loss = 0.79699577
Iteration 76, loss = 0.79463042
Iteration 77, loss = 0.79232502
Iteration 78, loss = 0.79006801
Iteration 79, loss = 0.78787401
Iteration 80, loss = 0.78572851
Iteration 81, loss = 0.78363919
Iteration 82, loss = 0.78159980
Iteration 83, loss = 0.77960321
Iteration 84, loss = 0.77766652
Iteration 85, loss = 0.77575538
Iteration 86, loss = 0.77390438
Iteration 87, loss = 0.77211012
Iteration 88, loss = 0.77046779
Iteration 89, loss = 0.76860156
Iteration 90, loss = 0.76689744
Iteration 91, loss = 0.76524001
Iteration 92, loss = 0.76361237
Iteration 93, loss = 0.76202882
Iteration 94, loss = 0.76048387
Iteration 95, loss = 0.75895581
Iteration 96, loss = 0.75747192
Iteration 97, loss = 0.75601663
Iteration 98, loss = 0.75458855
Iteration 99, loss = 0.75319653
Iteration 100, loss = 0.75183121
Iteration 101, loss = 0.75057923
Iteration 102, loss = 0.74918992
Iteration 103, loss = 0.74789378
Iteration 104, loss = 0.74662946
Iteration 105, loss = 0.74539177
Iteration 106, loss = 0.74417945
Iteration 107, loss = 0.74306461
Iteration 108, loss = 0.74182961
Iteration 109, loss = 0.74067514
Iteration 110, loss = 0.73954446
Iteration 111, loss = 0.73845250
Iteration 112, loss = 0.73742736
Iteration 113, loss = 0.73629599
Iteration 114, loss = 0.73526569
Iteration 115, loss = 0.73427921
Iteration 116, loss = 0.73321346
Iteration 117, loss = 0.73222484
Iteration 118, loss = 0.73130195
Iteration 119, loss = 0.73028539
Iteration 120, loss = 0.72938846
Iteration 121, loss = 0.72841554
Iteration 122, loss = 0.72754896
Iteration 123, loss = 0.72660429
Iteration 124, loss = 0.72572803
Iteration 125, loss = 0.72488743
Iteration 126, loss = 0.72399652
Iteration 127, loss = 0.72317936
Iteration 128, loss = 0.72231888
Iteration 129, loss = 0.72152761
Iteration 130, loss = 0.72069743
Iteration 131, loss = 0.71992082
Iteration 132, loss = 0.71911573
Iteration 133, loss = 0.71835789
Iteration 134, loss = 0.71758280
Iteration 135, loss = 0.71684053
Iteration 136, loss = 0.71609296
Iteration 137, loss = 0.71536289
Iteration 138, loss = 0.71464336
Iteration 139, loss = 0.71392382
Iteration 140, loss = 0.71323172
Iteration 141, loss = 0.71252121
Iteration 142, loss = 0.71185588
Iteration 143, loss = 0.71115320
Iteration 144, loss = 0.71051266
Iteration 145, loss = 0.70982218
Iteration 146, loss = 0.70916571
Iteration 147, loss = 0.70852298
Iteration 148, loss = 0.70791972
Iteration 149, loss = 0.70725995
Iteration 150, loss = 0.70663507
Iteration 151, loss = 0.70601946
Iteration 152, loss = 0.70541198
Iteration 153, loss = 0.70481066
Iteration 154, loss = 0.70421600
Iteration 155, loss = 0.70362757
Iteration 156, loss = 0.70304521
Iteration 157, loss = 0.70246858
Iteration 158, loss = 0.70189755
Iteration 159, loss = 0.70133202
Iteration 160, loss = 0.70077174
Iteration 161, loss = 0.70021666
Iteration 162, loss = 0.69966651
Iteration 163, loss = 0.69912119
Iteration 164, loss = 0.69858071
Iteration 165, loss = 0.69804458
Iteration 166, loss = 0.69751296
Iteration 167, loss = 0.69698569
Iteration 168, loss = 0.69646254
Iteration 169, loss = 0.69594350
Iteration 170, loss = 0.69542822
Iteration 171, loss = 0.69491678
Iteration 172, loss = 0.69440908
Iteration 173, loss = 0.69390467
Iteration 174, loss = 0.69340384
Iteration 175, loss = 0.69290619
Iteration 176, loss = 0.69241191
Iteration 177, loss = 0.69192073
Iteration 178, loss = 0.69143234
Iteration 179, loss = 0.69094704
Iteration 180, loss = 0.69046441
Iteration 181, loss = 0.68998458
Iteration 182, loss = 0.68950726
Iteration 183, loss = 0.68903261
Iteration 184, loss = 0.68856045
Iteration 185, loss = 0.68809041
Iteration 186, loss = 0.68762277
Iteration 187, loss = 0.68715729
Iteration 188, loss = 0.68669394
Iteration 189, loss = 0.68623254
Iteration 190, loss = 0.68577307
Iteration 191, loss = 0.68531540
Iteration 192, loss = 0.68485947
Iteration 193, loss = 0.68440524
Iteration 194, loss = 0.68395283
Iteration 195, loss = 0.68350220
Iteration 196, loss = 0.68305297
Iteration 197, loss = 0.68260528
Iteration 198, loss = 0.68215903
Iteration 199, loss = 0.68171439
Iteration 200, loss = 0.68127085
Iteration 201, loss = 0.68082873
Iteration 202, loss = 0.68038784
Iteration 203, loss = 0.67994813
Iteration 204, loss = 0.67950962
Iteration 205, loss = 0.67907223
Iteration 206, loss = 0.67863581
Iteration 207, loss = 0.67820051
Iteration 208, loss = 0.67776618
Iteration 209, loss = 0.67733278
Iteration 210, loss = 0.67690029
Iteration 211, loss = 0.67646880
Iteration 212, loss = 0.67603807
Iteration 213, loss = 0.67560809
Iteration 214, loss = 0.67517901
Iteration 215, loss = 0.67475072
Iteration 216, loss = 0.67432315
Iteration 217, loss = 0.67389620
Iteration 218, loss = 0.67347006
Iteration 219, loss = 0.67304451
Iteration 220, loss = 0.67261969
Iteration 221, loss = 0.67219559
Iteration 222, loss = 0.67176584
Iteration 223, loss = 0.67243017
Iteration 224, loss = 0.67518988
Iteration 225, loss = 0.67048622
Iteration 226, loss = 0.66939047
Iteration 227, loss = 0.66860004
Iteration 228, loss = 0.66796549
Iteration 229, loss = 0.66735603
Iteration 230, loss = 0.66675558
Iteration 231, loss = 0.66615714
Iteration 232, loss = 0.66556165
Iteration 233, loss = 0.66497183
Iteration 234, loss = 0.66438006
Iteration 235, loss = 0.66378596
Iteration 236, loss = 0.66318940
Iteration 237, loss = 0.66259085
Iteration 238, loss = 0.66198994
Iteration 239, loss = 0.66138762
Iteration 240, loss = 0.66078264
Iteration 241, loss = 0.66017532
Iteration 242, loss = 0.65956595
Iteration 243, loss = 0.65895483
Iteration 244, loss = 0.65834226
Iteration 245, loss = 0.65772750
Iteration 246, loss = 0.65711113
Iteration 247, loss = 0.65649319
Iteration 248, loss = 0.65587447
Iteration 249, loss = 0.65525518
Iteration 250, loss = 0.65463441
Iteration 251, loss = 0.65401275
Iteration 252, loss = 0.65339116
Iteration 253, loss = 0.65276988
Iteration 254, loss = 0.65229216
Iteration 255, loss = 0.65950989
Iteration 256, loss = 0.65819693
Iteration 257, loss = 0.67594609
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.3
Mean squared error = 0.9
Mean absolute error = 0.5
Median absolute error = 0.0
k_error=11.1636363636
____________________Fin K=8________________________

Pour k=9, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.12475756
Iteration 2, loss = 2.06801383
Iteration 3, loss = 1.99049869
Iteration 4, loss = 1.90817606
Iteration 5, loss = 1.81811160
Iteration 6, loss = 1.70964180
Iteration 7, loss = 1.57688880
Iteration 8, loss = 1.49467613
Iteration 9, loss = 1.45051310
Iteration 10, loss = 1.40017249
Iteration 11, loss = 1.35263741
Iteration 12, loss = 1.31141448
Iteration 13, loss = 1.27477017
Iteration 14, loss = 1.24307062
Iteration 15, loss = 1.21567161
Iteration 16, loss = 1.19169126
Iteration 17, loss = 1.17061408
Iteration 18, loss = 1.15120372
Iteration 19, loss = 1.13322453
Iteration 20, loss = 1.11640176
Iteration 21, loss = 1.10059180
Iteration 22, loss = 1.08578797
Iteration 23, loss = 1.07166624
Iteration 24, loss = 1.05828903
Iteration 25, loss = 1.04533051
Iteration 26, loss = 1.03294911
Iteration 27, loss = 1.02117441
Iteration 28, loss = 1.01020230
Iteration 29, loss = 0.99942787
Iteration 30, loss = 0.98933441
Iteration 31, loss = 0.97975791
Iteration 32, loss = 0.97065787
Iteration 33, loss = 0.96211019
Iteration 34, loss = 0.95321396
Iteration 35, loss = 0.94515024
Iteration 36, loss = 0.93696945
Iteration 37, loss = 0.92925547
Iteration 38, loss = 0.92231424
Iteration 39, loss = 0.91620132
Iteration 40, loss = 0.90817697
Iteration 41, loss = 0.90139595
Iteration 42, loss = 0.89487766
Iteration 43, loss = 0.88874070
Iteration 44, loss = 0.88264171
Iteration 45, loss = 0.87706740
Iteration 46, loss = 0.87101315
Iteration 47, loss = 0.86541888
Iteration 48, loss = 0.86002249
Iteration 49, loss = 0.85483196
Iteration 50, loss = 0.84987912
Iteration 51, loss = 0.84486512
Iteration 52, loss = 0.84009156
Iteration 53, loss = 0.83546967
Iteration 54, loss = 0.83102522
Iteration 55, loss = 0.82681126
Iteration 56, loss = 0.82247884
Iteration 57, loss = 0.81837630
Iteration 58, loss = 0.81441105
Iteration 59, loss = 0.81056785
Iteration 60, loss = 0.80698105
Iteration 61, loss = 0.80325925
Iteration 62, loss = 0.79973265
Iteration 63, loss = 0.79632238
Iteration 64, loss = 0.79301211
Iteration 65, loss = 0.78979813
Iteration 66, loss = 0.78670929
Iteration 67, loss = 0.78379182
Iteration 68, loss = 0.78074889
Iteration 69, loss = 0.77787300
Iteration 70, loss = 0.77509088
Iteration 71, loss = 0.77241754
Iteration 72, loss = 0.77007930
Iteration 73, loss = 0.76729100
Iteration 74, loss = 0.76477013
Iteration 75, loss = 0.76233985
Iteration 76, loss = 0.75997445
Iteration 77, loss = 0.75771399
Iteration 78, loss = 0.75572144
Iteration 79, loss = 0.75332944
Iteration 80, loss = 0.75117974
Iteration 81, loss = 0.74910330
Iteration 82, loss = 0.74710159
Iteration 83, loss = 0.74540334
Iteration 84, loss = 0.74331088
Iteration 85, loss = 0.74149317
Iteration 86, loss = 0.73957583
Iteration 87, loss = 0.73777469
Iteration 88, loss = 0.73601789
Iteration 89, loss = 0.73434723
Iteration 90, loss = 0.73291729
Iteration 91, loss = 0.73111720
Iteration 92, loss = 0.72972262
Iteration 93, loss = 0.72797363
Iteration 94, loss = 0.72640949
Iteration 95, loss = 0.72490488
Iteration 96, loss = 0.72348206
Iteration 97, loss = 0.72225756
Iteration 98, loss = 0.72070081
Iteration 99, loss = 0.71949537
Iteration 100, loss = 0.71798761
Iteration 101, loss = 0.71664363
Iteration 102, loss = 0.71537418
Iteration 103, loss = 0.71429183
Iteration 104, loss = 0.71291431
Iteration 105, loss = 0.71184792
Iteration 106, loss = 0.71051655
Iteration 107, loss = 0.70932660
Iteration 108, loss = 0.70828761
Iteration 109, loss = 0.70710689
Iteration 110, loss = 0.70613046
Iteration 111, loss = 0.70492152
Iteration 112, loss = 0.70386120
Iteration 113, loss = 0.70295348
Iteration 114, loss = 0.70180917
Iteration 115, loss = 0.70091616
Iteration 116, loss = 0.69982883
Iteration 117, loss = 0.69886412
Iteration 118, loss = 0.69801762
Iteration 119, loss = 0.69698246
Iteration 120, loss = 0.69615099
Iteration 121, loss = 0.69516263
Iteration 122, loss = 0.69426171
Iteration 123, loss = 0.69343369
Iteration 124, loss = 0.69262575
Iteration 125, loss = 0.69173349
Iteration 126, loss = 0.69094128
Iteration 127, loss = 0.69007816
Iteration 128, loss = 0.68932374
Iteration 129, loss = 0.68848096
Iteration 130, loss = 0.68775309
Iteration 131, loss = 0.68693875
Iteration 132, loss = 0.68623270
Iteration 133, loss = 0.68544458
Iteration 134, loss = 0.68475619
Iteration 135, loss = 0.68399750
Iteration 136, loss = 0.68332257
Iteration 137, loss = 0.68259309
Iteration 138, loss = 0.68193383
Iteration 139, loss = 0.68123107
Iteration 140, loss = 0.68058455
Iteration 141, loss = 0.67990368
Iteration 142, loss = 0.67927240
Iteration 143, loss = 0.67861871
Iteration 144, loss = 0.67799990
Iteration 145, loss = 0.67737515
Iteration 146, loss = 0.67809035
Iteration 147, loss = 0.67635909
Iteration 148, loss = 0.67571961
Iteration 149, loss = 0.67506054
Iteration 150, loss = 0.67442752
Iteration 151, loss = 0.67382628
Iteration 152, loss = 0.67326950
Iteration 153, loss = 0.67270610
Iteration 154, loss = 0.67213846
Iteration 155, loss = 0.67161784
Iteration 156, loss = 0.67105032
Iteration 157, loss = 0.67051924
Iteration 158, loss = 0.67002549
Iteration 159, loss = 0.66947409
Iteration 160, loss = 0.66896139
Iteration 161, loss = 0.66848635
Iteration 162, loss = 0.66795614
Iteration 163, loss = 0.66746011
Iteration 164, loss = 0.66697089
Iteration 165, loss = 0.66648808
Iteration 166, loss = 0.66601097
Iteration 167, loss = 0.66553931
Iteration 168, loss = 0.66507288
Iteration 169, loss = 0.66461156
Iteration 170, loss = 0.66415517
Iteration 171, loss = 0.66370354
Iteration 172, loss = 0.66325659
Iteration 173, loss = 0.66281418
Iteration 174, loss = 0.66237627
Iteration 175, loss = 0.66194264
Iteration 176, loss = 0.66151320
Iteration 177, loss = 0.66108784
Iteration 178, loss = 0.66066645
Iteration 179, loss = 0.66024900
Iteration 180, loss = 0.65983525
Iteration 181, loss = 0.65942526
Iteration 182, loss = 0.65901869
Iteration 183, loss = 0.65861560
Iteration 184, loss = 0.65821588
Iteration 185, loss = 0.65781941
Iteration 186, loss = 0.65742610
Iteration 187, loss = 0.65703577
Iteration 188, loss = 0.65664839
Iteration 189, loss = 0.65626385
Iteration 190, loss = 0.65588213
Iteration 191, loss = 0.65550304
Iteration 192, loss = 0.65512649
Iteration 193, loss = 0.65475240
Iteration 194, loss = 0.65438068
Iteration 195, loss = 0.65401129
Iteration 196, loss = 0.65364410
Iteration 197, loss = 0.65327901
Iteration 198, loss = 0.65291576
Iteration 199, loss = 0.65255426
Iteration 200, loss = 0.65219463
Iteration 201, loss = 0.65183672
Iteration 202, loss = 0.65148048
Iteration 203, loss = 0.65112584
Iteration 204, loss = 0.65077273
Iteration 205, loss = 0.65042109
Iteration 206, loss = 0.65007081
Iteration 207, loss = 0.64972179
Iteration 208, loss = 0.64937399
Iteration 209, loss = 0.64902733
Iteration 210, loss = 0.64868172
Iteration 211, loss = 0.64833711
Iteration 212, loss = 0.64799341
Iteration 213, loss = 0.64765051
Iteration 214, loss = 0.64730836
Iteration 215, loss = 0.64696688
Iteration 216, loss = 0.64662601
Iteration 217, loss = 0.64628568
Iteration 218, loss = 0.64594582
Iteration 219, loss = 0.64560639
Iteration 220, loss = 0.64526728
Iteration 221, loss = 0.64492847
Iteration 222, loss = 0.64458992
Iteration 223, loss = 0.64425153
Iteration 224, loss = 0.64391323
Iteration 225, loss = 0.64362757
Iteration 226, loss = 0.64574436
Iteration 227, loss = 0.64320626
Iteration 228, loss = 0.64276457
Iteration 229, loss = 0.64235587
Iteration 230, loss = 0.64199120
Iteration 231, loss = 0.64164162
Iteration 232, loss = 0.64129960
Iteration 233, loss = 0.64096013
Iteration 234, loss = 0.64062085
Iteration 235, loss = 0.64028111
Iteration 236, loss = 0.63994090
Iteration 237, loss = 0.63960028
Iteration 238, loss = 0.63925920
Iteration 239, loss = 0.63891760
Iteration 240, loss = 0.63857537
Iteration 241, loss = 0.63823247
Iteration 242, loss = 0.63788885
Iteration 243, loss = 0.63754447
Iteration 244, loss = 0.63719932
Iteration 245, loss = 0.63685334
Iteration 246, loss = 0.63650652
Iteration 247, loss = 0.63615881
Iteration 248, loss = 0.63581020
Iteration 249, loss = 0.63546068
Iteration 250, loss = 0.63511020
Iteration 251, loss = 0.63475875
Iteration 252, loss = 0.63440632
Iteration 253, loss = 0.63405289
Iteration 254, loss = 0.63369845
Iteration 255, loss = 0.63334298
Iteration 256, loss = 0.63298654
Iteration 257, loss = 0.63262910
Iteration 258, loss = 0.63227061
Iteration 259, loss = 0.63191106
Iteration 260, loss = 0.63155044
Iteration 261, loss = 0.63118874
Iteration 262, loss = 0.63082598
Iteration 263, loss = 0.63053060
Iteration 264, loss = 0.63424312
Iteration 265, loss = 0.63063203
Iteration 266, loss = 0.62964861
Iteration 267, loss = 0.62918127
Iteration 268, loss = 0.62876417
Iteration 269, loss = 0.62838161
Iteration 270, loss = 0.62800574
Iteration 271, loss = 0.62763165
Iteration 272, loss = 0.62725741
Iteration 273, loss = 0.62688304
Iteration 274, loss = 0.62650835
Iteration 275, loss = 0.62613312
Iteration 276, loss = 0.62575705
Iteration 277, loss = 0.62538019
Iteration 278, loss = 0.62500250
Iteration 279, loss = 0.62462394
Iteration 280, loss = 0.62424456
Iteration 281, loss = 0.62386454
Iteration 282, loss = 0.62348362
Iteration 283, loss = 0.62310202
Iteration 284, loss = 0.62271982
Iteration 285, loss = 0.62233688
Iteration 286, loss = 0.62195332
Iteration 287, loss = 0.62156920
Iteration 288, loss = 0.62118456
Iteration 289, loss = 0.62079936
Iteration 290, loss = 0.62041366
Iteration 291, loss = 0.62002759
Iteration 292, loss = 0.61964109
Iteration 293, loss = 0.61925422
Iteration 294, loss = 0.61886704
Iteration 295, loss = 0.61847968
Iteration 296, loss = 0.61809198
Iteration 297, loss = 0.61770413
Iteration 298, loss = 0.61731618
Iteration 299, loss = 0.61692815
Iteration 300, loss = 0.61654006
Iteration 301, loss = 0.61615197
Iteration 302, loss = 0.61576395
Iteration 303, loss = 0.61537608
Iteration 304, loss = 0.61498833
Iteration 305, loss = 0.61460078
Iteration 306, loss = 0.61421360
Iteration 307, loss = 0.61382669
Iteration 308, loss = 0.61344014
Iteration 309, loss = 0.61306898
Iteration 310, loss = 0.61997346
Iteration 311, loss = 0.61612029
Iteration 312, loss = 0.61542436
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.4
Mean squared error = 3.5
Mean absolute error = 1.1
Median absolute error = 0.0
k_error=14.6636363636
____________________Fin K=9________________________

Pour k=10, nous avons les resultats suivants
_________________________________________________________

MLPClassifier(activation='relu', algorithm='sgd', alpha=0.0001,
       batch_size='auto', beta_1=0.9, beta_2=0.999, early_stopping=False,
       epsilon=1e-08, hidden_layer_sizes=(20, 1), learning_rate='constant',
       learning_rate_init=0.2, max_iter=500, momentum=0.6,
       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,
       tol=0.0001, validation_fraction=0.1, verbose=True, warm_start=False)
Iteration 1, loss = 2.11997337
Iteration 2, loss = 2.06656116
Iteration 3, loss = 1.99847961
Iteration 4, loss = 1.92440939
Iteration 5, loss = 1.84560669
Iteration 6, loss = 1.75090005
Iteration 7, loss = 1.62728646
Iteration 8, loss = 1.53653655
Iteration 9, loss = 1.49095948
Iteration 10, loss = 1.44406549
Iteration 11, loss = 1.39865864
Iteration 12, loss = 1.35946514
Iteration 13, loss = 1.32403222
Iteration 14, loss = 1.29166598
Iteration 15, loss = 1.26302057
Iteration 16, loss = 1.23744786
Iteration 17, loss = 1.21443152
Iteration 18, loss = 1.19322765
Iteration 19, loss = 1.17355766
Iteration 20, loss = 1.15533566
Iteration 21, loss = 1.13824839
Iteration 22, loss = 1.12233466
Iteration 23, loss = 1.10731803
Iteration 24, loss = 1.09310934
Iteration 25, loss = 1.07978345
Iteration 26, loss = 1.06717558
Iteration 27, loss = 1.05518077
Iteration 28, loss = 1.04368088
Iteration 29, loss = 1.03258180
Iteration 30, loss = 1.02206850
Iteration 31, loss = 1.01233867
Iteration 32, loss = 1.00322119
Iteration 33, loss = 0.99366560
Iteration 34, loss = 0.98518809
Iteration 35, loss = 0.97661623
Iteration 36, loss = 0.96846289
Iteration 37, loss = 0.96065342
Iteration 38, loss = 0.95308945
Iteration 39, loss = 0.94583659
Iteration 40, loss = 0.93882906
Iteration 41, loss = 0.93231609
Iteration 42, loss = 0.92536927
Iteration 43, loss = 0.91888882
Iteration 44, loss = 0.91270963
Iteration 45, loss = 0.90703057
Iteration 46, loss = 0.90087089
Iteration 47, loss = 0.89515735
Iteration 48, loss = 0.88963715
Iteration 49, loss = 0.88429992
Iteration 50, loss = 0.87920304
Iteration 51, loss = 0.87412503
Iteration 52, loss = 0.86924617
Iteration 53, loss = 0.86452843
Iteration 54, loss = 0.86001892
Iteration 55, loss = 0.85552904
Iteration 56, loss = 0.85120885
Iteration 57, loss = 0.84704264
Iteration 58, loss = 0.84302416
Iteration 59, loss = 0.83915005
Iteration 60, loss = 0.83530349
Iteration 61, loss = 0.83164613
Iteration 62, loss = 0.82801997
Iteration 63, loss = 0.82451222
Iteration 64, loss = 0.82111537
Iteration 65, loss = 0.81782089
Iteration 66, loss = 0.81470607
Iteration 67, loss = 0.81152346
Iteration 68, loss = 0.80849350
Iteration 69, loss = 0.80558932
Iteration 70, loss = 0.80273178
Iteration 71, loss = 0.79990841
Iteration 72, loss = 0.79721709
Iteration 73, loss = 0.79454653
Iteration 74, loss = 0.79204260
Iteration 75, loss = 0.78947597
Iteration 76, loss = 0.78703513
Iteration 77, loss = 0.78469707
Iteration 78, loss = 0.78238803
Iteration 79, loss = 0.78014578
Iteration 80, loss = 0.77794270
Iteration 81, loss = 0.77579972
Iteration 82, loss = 0.77371346
Iteration 83, loss = 0.77166633
Iteration 84, loss = 0.76967924
Iteration 85, loss = 0.76773192
Iteration 86, loss = 0.76583308
Iteration 87, loss = 0.76397691
Iteration 88, loss = 0.76216208
Iteration 89, loss = 0.76039736
Iteration 90, loss = 0.75865911
Iteration 91, loss = 0.75696258
Iteration 92, loss = 0.75530564
Iteration 93, loss = 0.75369184
Iteration 94, loss = 0.75211204
Iteration 95, loss = 0.75056928
Iteration 96, loss = 0.74903699
Iteration 97, loss = 0.74757893
Iteration 98, loss = 0.74634037
Iteration 99, loss = 0.74472707
Iteration 100, loss = 0.74329507
Iteration 101, loss = 0.74191522
Iteration 102, loss = 0.74057438
Iteration 103, loss = 0.73925842
Iteration 104, loss = 0.73797325
Iteration 105, loss = 0.73671453
Iteration 106, loss = 0.73546414
Iteration 107, loss = 0.73424896
Iteration 108, loss = 0.73305521
Iteration 109, loss = 0.73188971
Iteration 110, loss = 0.73075271
Iteration 111, loss = 0.72961505
Iteration 112, loss = 0.72850939
Iteration 113, loss = 0.72742335
Iteration 114, loss = 0.72635591
Iteration 115, loss = 0.72531091
Iteration 116, loss = 0.72428079
Iteration 117, loss = 0.72326935
Iteration 118, loss = 0.72227672
Iteration 119, loss = 0.72129854
Iteration 120, loss = 0.72033651
Iteration 121, loss = 0.71939054
Iteration 122, loss = 0.71846158
Iteration 123, loss = 0.71755618
Iteration 124, loss = 0.71674127
Iteration 125, loss = 0.71578081
Iteration 126, loss = 0.71489776
Iteration 127, loss = 0.71403934
Iteration 128, loss = 0.71319460
Iteration 129, loss = 0.71236553
Iteration 130, loss = 0.71162779
Iteration 131, loss = 0.71075722
Iteration 132, loss = 0.70995530
Iteration 133, loss = 0.70917253
Iteration 134, loss = 0.70841705
Iteration 135, loss = 0.70771348
Iteration 136, loss = 0.70691531
Iteration 137, loss = 0.70623369
Iteration 138, loss = 0.70545883
Iteration 139, loss = 0.70473584
Iteration 140, loss = 0.70402673
Iteration 141, loss = 0.70333703
Iteration 142, loss = 0.70270305
Iteration 143, loss = 0.70198210
Iteration 144, loss = 0.70135919
Iteration 145, loss = 0.70066191
Iteration 146, loss = 0.70000491
Iteration 147, loss = 0.69936817
Iteration 148, loss = 0.69878108
Iteration 149, loss = 0.69811665
Iteration 150, loss = 0.69754032
Iteration 151, loss = 0.69689715
Iteration 152, loss = 0.69629031
Iteration 153, loss = 0.69574396
Iteration 154, loss = 0.69511759
Iteration 155, loss = 0.69454412
Iteration 156, loss = 0.69400800
Iteration 157, loss = 0.69341226
Iteration 158, loss = 0.69288435
Iteration 159, loss = 0.69230143
Iteration 160, loss = 0.69179024
Iteration 161, loss = 0.69122001
Iteration 162, loss = 0.69072137
Iteration 163, loss = 0.69016536
Iteration 164, loss = 0.68967488
Iteration 165, loss = 0.68913449
Iteration 166, loss = 0.68865123
Iteration 167, loss = 0.68812654
Iteration 168, loss = 0.68764799
Iteration 169, loss = 0.68713910
Iteration 170, loss = 0.68666767
Iteration 171, loss = 0.68617276
Iteration 172, loss = 0.68570629
Iteration 173, loss = 0.68522457
Iteration 174, loss = 0.68476330
Iteration 175, loss = 0.68429561
Iteration 176, loss = 0.68383838
Iteration 177, loss = 0.68338432
Iteration 178, loss = 0.68293062
Iteration 179, loss = 0.68249018
Iteration 180, loss = 0.68203943
Iteration 181, loss = 0.68161118
Iteration 182, loss = 0.68116220
Iteration 183, loss = 0.68074823
Iteration 184, loss = 0.68030191
Iteration 185, loss = 0.67989986
Iteration 186, loss = 0.67945485
Iteration 187, loss = 0.67906473
Iteration 188, loss = 0.67862570
Iteration 189, loss = 0.67821026
Iteration 190, loss = 0.67780320
Iteration 191, loss = 0.67739980
Iteration 192, loss = 0.67699963
Iteration 193, loss = 0.67660248
Iteration 194, loss = 0.67620824
Iteration 195, loss = 0.67581685
Iteration 196, loss = 0.67542819
Iteration 197, loss = 0.67504226
Iteration 198, loss = 0.67465884
Iteration 199, loss = 0.67427793
Iteration 200, loss = 0.67389942
Iteration 201, loss = 0.67352331
Iteration 202, loss = 0.67314942
Iteration 203, loss = 0.67277770
Iteration 204, loss = 0.67240810
Iteration 205, loss = 0.67204064
Iteration 206, loss = 0.67167507
Iteration 207, loss = 0.67131143
Iteration 208, loss = 0.67094970
Iteration 209, loss = 0.67058975
Iteration 210, loss = 0.67024345
Iteration 211, loss = 0.67262149
Iteration 212, loss = 0.67047108
Iteration 213, loss = 0.66983506
Iteration 214, loss = 0.66915613
Iteration 215, loss = 0.66863820
Iteration 216, loss = 0.66824666
Iteration 217, loss = 0.66788862
Iteration 218, loss = 0.66753511
Iteration 219, loss = 0.66718568
Iteration 220, loss = 0.66684040
Iteration 221, loss = 0.66649839
Iteration 222, loss = 0.66615833
Iteration 223, loss = 0.66581950
Iteration 224, loss = 0.66548167
Iteration 225, loss = 0.66514482
Iteration 226, loss = 0.66480894
Iteration 227, loss = 0.66447396
Iteration 228, loss = 0.66413984
Iteration 229, loss = 0.66380652
Iteration 230, loss = 0.66347395
Iteration 231, loss = 0.66314208
Iteration 232, loss = 0.66281088
Iteration 233, loss = 0.66248030
Iteration 234, loss = 0.66215030
Iteration 235, loss = 0.66182089
Iteration 236, loss = 0.66149204
Iteration 237, loss = 0.66116366
Iteration 238, loss = 0.66083572
Iteration 239, loss = 0.66050817
Iteration 240, loss = 0.66018099
Iteration 241, loss = 0.65985413
Iteration 242, loss = 0.65952757
Iteration 243, loss = 0.65920127
Iteration 244, loss = 0.65887520
Iteration 245, loss = 0.65854934
Iteration 246, loss = 0.65822365
Iteration 247, loss = 0.65789812
Iteration 248, loss = 0.65757268
Iteration 249, loss = 0.65724734
Iteration 250, loss = 0.65692205
Iteration 251, loss = 0.65659681
Iteration 252, loss = 0.65627158
Iteration 253, loss = 0.65594634
Iteration 254, loss = 0.65562121
Iteration 255, loss = 0.65529587
Iteration 256, loss = 0.65497055
Iteration 257, loss = 0.65464515
Iteration 258, loss = 0.65431960
Iteration 259, loss = 0.65399393
Iteration 260, loss = 0.65366813
Iteration 261, loss = 0.65334219
Iteration 262, loss = 0.65301604
Iteration 263, loss = 0.65268970
Iteration 264, loss = 0.65236320
Iteration 265, loss = 0.65203645
Iteration 266, loss = 0.65170948
Iteration 267, loss = 0.65138226
Iteration 268, loss = 0.65105485
Iteration 269, loss = 0.65072713
Iteration 270, loss = 0.65044622
Iteration 271, loss = 0.65682862
Iteration 272, loss = 0.65184957
Iteration 273, loss = 0.64960109
Iteration 274, loss = 0.64892188
Iteration 275, loss = 0.64837465
Iteration 276, loss = 0.64788029
Iteration 277, loss = 0.64743942
Iteration 278, loss = 0.64701919
Iteration 279, loss = 0.64673292
Iteration 280, loss = 0.64643287
Iteration 281, loss = 0.64640544
Iteration 282, loss = 0.64629546
Iteration 283, loss = 0.64690697
Iteration 284, loss = 0.64727001
Iteration 285, loss = 0.66347887
Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.
Error rate= 0.3
Mean squared error = 2.2
Mean absolute error = 0.8
Median absolute error = 0.0
k_error=16.8636363636
____________________Fin K=10________________________

Moyenne des k erreurs quadratique est: 1.68636363636
